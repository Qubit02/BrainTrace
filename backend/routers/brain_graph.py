"""
brain_graph.py

ë¸Œë ˆì¸ ê·¸ë˜í”„ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” ë¼ìš°í„° ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ë¸Œë ˆì¸ ê·¸ë˜í”„ ë°ì´í„° ì¡°íšŒ ë° ê´€ë¦¬
- í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ê·¸ë˜í”„ ìƒì„±
- ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ
- ì†ŒìŠ¤ ë°ì´í„° ë©”íŠ¸ë¦­ ë° í†µê³„
- ë…¸ë“œ-ì†ŒìŠ¤ ê´€ê³„ ê´€ë¦¬

ì§€ì›í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸:
- GET /brainGraph/getNodeEdge/{brain_id} : ë¸Œë ˆì¸ì˜ ê·¸ë˜í”„ ë°ì´í„° ì¡°íšŒ
- POST /brainGraph/process_text : í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ê·¸ë˜í”„ ìƒì„±
- POST /brainGraph/answer : ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
- GET /brainGraph/getSourceIds : ë…¸ë“œì˜ ëª¨ë“  source_idì™€ ì œëª© ì¡°íšŒ
- GET /brainGraph/getNodesBySourceId : source_idë¡œ ë…¸ë“œ ì¡°íšŒ
- GET /brainGraph/getSourceDataMetrics/{brain_id} : ë¸Œë ˆì¸ì˜ ì†ŒìŠ¤ë³„ ë°ì´í„° ë©”íŠ¸ë¦­ ì¡°íšŒ
- GET /brainGraph/sourceCount/{brain_id} : ë¸Œë ˆì¸ë³„ ì „ì²´ ì†ŒìŠ¤ ê°œìˆ˜ ì¡°íšŒ
- GET /brainGraph/getSourceContent : ì†ŒìŠ¤ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ë‚´ìš© ì¡°íšŒ

ê·¸ë˜í”„ ì²˜ë¦¬:
- Neo4j: ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤ (ë…¸ë“œ, ì—£ì§€, ê´€ê³„)
- Qdrant: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (ì„ë² ë”© ê²€ìƒ‰)
- AI ëª¨ë¸: GPT/Ollamaë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬

ë°ì´í„°ë² ì´ìŠ¤:
- Neo4j: ê·¸ë˜í”„ ë°ì´í„° ì €ì¥ ë° ì¡°íšŒ
- Qdrant: ë²¡í„° ì„ë² ë”© ì €ì¥ ë° ìœ ì‚¬ë„ ê²€ìƒ‰
- SQLite: ì†ŒìŠ¤ íŒŒì¼ ì •ë³´ ë° ë©”íƒ€ë°ì´í„°

ì˜ì¡´ì„±:
- FastAPI: ì›¹ í”„ë ˆì„ì›Œí¬
- Neo4jHandler: Neo4j ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—…
- embedding_service: ë²¡í„° ì„ë² ë”© ë° ê²€ìƒ‰
- AI ì„œë¹„ìŠ¤: GPT/Ollama ëª¨ë¸ ì—°ë™
- SQLiteHandler: SQLite ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—…

ì‘ì„±ì: BrainT ê°œë°œíŒ€
ìµœì¢… ìˆ˜ì •ì¼: 2024ë…„
"""

from fastapi import APIRouter, HTTPException
from models.request_models import ProcessTextRequest, AnswerRequest, GraphResponse
from services import embedding_service
from neo4j_db.Neo4jHandler import Neo4jHandler
import logging
from sqlite_db import SQLiteHandler
from exceptions.custom_exceptions import Neo4jException,AppException, QdrantException
from examples.error_examples import ErrorExamples
from dependencies import get_ai_service_GPT
from dependencies import get_ai_service_Ollama
from services.accuracy_service import compute_accuracy
from services import manual_chunking_sentences
import time
import json
import re

# LangChain imports
try:
    from langchain_core.pydantic_v1 import BaseModel, Field
    from langchain.tools import StructuredTool
    try:
        from langchain_openai import ChatOpenAI
    except ImportError:
        from langchain_community.chat_models import ChatOpenAI
    try:
        from langchain_community.chat_models import ChatOllama
    except ImportError:
        ChatOllama = None
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    logging.warning(f"LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì¼ë¶€ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¤ìŠ¤í…€ Agentë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI ë¼ìš°í„° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
router = APIRouter(
    prefix="/brainGraph",
    tags=["brainGraph"],
    responses={404: {"description": "Not found"}}
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ API ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€ #

@router.get(
    "/getNodeEdge/{brain_id}",
    response_model=GraphResponse,
    summary="ë¸Œë ˆì¸ì˜ ê·¸ë˜í”„ ë°ì´í„° ì¡°íšŒ",
    description="íŠ¹ì • ë¸Œë ˆì¸ì˜ ëª¨ë“  ë…¸ë“œì™€ ì—£ì§€(ê´€ê³„) ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
     responses={
        404: ErrorExamples[40401],
        500: ErrorExamples[50001]
    }
    
)
async def get_brain_graph(brain_id: str):
    """
    íŠ¹ì • ë¸Œë ˆì¸ì˜ ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ì²˜ë¦¬ ê³¼ì •:
    1. brain_idë¡œ Neo4jì—ì„œ ê·¸ë˜í”„ ë°ì´í„° ì¡°íšŒ
    2. ë…¸ë“œì™€ ì—£ì§€ ì •ë³´ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ë°˜í™˜
    3. ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ê·¸ë˜í”„ ë°˜í™˜
    
    Args:
        brain_id (str): ê·¸ë˜í”„ë¥¼ ì¡°íšŒí•  ë¸Œë ˆì¸ ID
    
    Returns:
        GraphResponse: ê·¸ë˜í”„ ë°ì´í„°
            - nodes: ë…¸ë“œ ëª©ë¡ (ê° ë…¸ë“œëŠ” name ì†ì„±ì„ ê°€ì§)
            - links: ì—£ì§€ ëª©ë¡ (ê° ì—£ì§€ëŠ” source, target, relation ì†ì„±ì„ ê°€ì§)
    
    Raises:
        Neo4jException: Neo4j ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜
    """
    # ìš”ì²­ ìˆ˜ì‹  ë¡œê¹…: ê´€ì¸¡/ë””ë²„ê¹…ì„ ìœ„í•´ ì£¼ìš” íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ë¡
    logging.info(f"getNodeEdge ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ - brain_id: {brain_id}")
    try:
        # ê·¸ë˜í”„ DB í•¸ë“¤ëŸ¬ ìƒì„± (ê° ìš”ì²­ë§ˆë‹¤ ë…ë¦½ ìƒì„±í•˜ì—¬ ì„¸ì…˜ ìˆ˜ëª… ê´€ë¦¬)
        neo4j_handler = Neo4jHandler()
        logging.info("Neo4j í•¸ë“¤ëŸ¬ ìƒì„±ë¨")
        
        # Neo4jì—ì„œ ë¸Œë ˆì¸ ì „ì²´ ê·¸ë˜í”„ ì¡°íšŒ
        graph_data = neo4j_handler.get_brain_graph(brain_id)
        logging.info(f"Neo4jì—ì„œ ë°›ì€ ë°ì´í„°: nodes={len(graph_data['nodes'])}, links={len(graph_data['links'])}")
        
        # if not graph_data['nodes'] and not graph_data['links']:
        #     logging.warning(f"brain_id {brain_id}ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        #     raise GraphDataNotFoundException(brain_id)
        
        if not graph_data['nodes'] and not graph_data['links']:
            logging.warning(f"brain_id {brain_id}ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

        return graph_data
    except AppException as ae:
            raise ae
    except Exception as e:
        logging.error("ê·¸ë˜í”„ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: %s", str(e))
        raise Neo4jException(message=str(e))
        

@router.post("/process_text", 
    summary="í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ê·¸ë˜í”„ ìƒì„±",
    description= """
    í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë…¸ë“œ/ì—£ì§€ ì¶”ì¶œ, Neo4j ì €ì¥, ë²¡í„° DB ì„ë² ë”©ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    <br> Ollama ì‚¬ìš© â†’ (model: "ollama")  
    <br> GPT ì‚¬ìš© â†’ (model: "gpt")
    """,
    response_description="ì²˜ë¦¬ëœ ë…¸ë“œì™€ ì—£ì§€ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
    responses={
        500: ErrorExamples[50001]
    }
    )
async def process_text_endpoint(request_data: ProcessTextRequest):
    """
    í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë…¸ë“œ/ì—£ì§€ ì¶”ì¶œ, Neo4j ì €ì¥, ë²¡í„° DB ì„ë² ë”©ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    ì²˜ë¦¬ ê³¼ì •:
    1. í…ìŠ¤íŠ¸ ì…ë ¥ ê²€ì¦
    2. AI ëª¨ë¸ ì„ íƒ (GPT ë˜ëŠ” Ollama)
    3. í…ìŠ¤íŠ¸ì—ì„œ ë…¸ë“œì™€ ì—£ì§€ ì¶”ì¶œ
    4. Neo4jì— ê·¸ë˜í”„ ë°ì´í„° ì €ì¥
    5. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì„ë² ë”© ì €ì¥
    6. ì²˜ë¦¬ ê²°ê³¼ ë°˜í™˜
    
    Args:
        request_data (ProcessTextRequest): í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìš”ì²­
            - text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            - source_id: ì†ŒìŠ¤ ID
            - brain_id: ë¸Œë ˆì¸ ID
            - model: ì‚¬ìš©í•  ëª¨ë¸ ("gpt" ë˜ëŠ” "ollama")
    
    Returns:
        dict: ì²˜ë¦¬ëœ ë…¸ë“œì™€ ì—£ì§€ ì •ë³´
    
    Raises:
        HTTPException: 
            - 400: í•„ìˆ˜ íŒŒë¼ë¯¸í„° ëˆ„ë½
            - 500: ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ
    """
    # ìš”ì²­ ë³¸ë¬¸ íŒŒë¼ë¯¸í„° ì–¸íŒ¨í‚¹
    text = request_data.text
    source_id = request_data.source_id
    brain_id = request_data.brain_id
    model = None
    t0 = time.perf_counter()
    # ë¡œê¹…ì€ í¬ë§· ë¬¸ìì—´ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ ê¶Œì¥ë©ë‹ˆë‹¤ (ì—¬ê¸°ì„  ê¸°ì¡´ ìŠ¤íƒ€ì¼ ìœ ì§€)
    logging.info('model : %s', model)
    if not text:
        raise HTTPException(status_code=400, detail="text íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    if not source_id:
        raise HTTPException(status_code=400, detail="source_id íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    if not brain_id:
        raise HTTPException(status_code=400, detail="brain_id íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ë³´ì•ˆ/í”„ë¼ì´ë²„ì‹œ ê³ ë ¤: ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì›ë¬¸ ì „ì²´ë¥¼ ë¡œê·¸ì— ë‚¨ê¸°ì§€ ì•ŠëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.
    logging.info("ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸: %s, source_id: %s, brain_id: %s", text, source_id, brain_id)
    
    # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì£¼ì…
    if model == "gpt":
        ai_service = get_ai_service_GPT()
    elif model == "ollama":
        ai_service = get_ai_service_Ollama()
    else:
        ai_service=None
    
    # ë…¸ë“œ ì •ë³´ë¥¼ ë²¡í„° DBì— ì„ë² ë”©
    # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
    if not embedding_service.is_index_ready(brain_id):
        embedding_service.initialize_collection(brain_id)
    
    # Step 1: í…ìŠ¤íŠ¸ì—ì„œ ë…¸ë“œ/ì—£ì§€ ì¶”ì¶œ (AI ì„œë¹„ìŠ¤)
    if ai_service==None:
        # ê¸°ë³¸ ìˆ˜ë™ ì²­í¬ ì²˜ë¦¬: ëª¨ë¸ ë¯¸ì„ íƒ ì‹œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë…¸ë“œ/ì—£ì§€ ì¶”ì¶œ
        nodes, edges=manual_chunking_sentences.extract_graph_components(text, (brain_id, source_id))
    else:
        # ì„ íƒëœ AI ì„œë¹„ìŠ¤ê°€ ì œê³µí•˜ëŠ” ì¶”ì¶œ ë¡œì§ í˜¸ì¶œ
        nodes, edges = ai_service.extract_graph_components(text, source_id)
    logging.info("ì¶”ì¶œëœ ë…¸ë“œ: %s", nodes)
    logging.info("ì¶”ì¶œëœ ì—£ì§€: %s", edges)

    # Step 2: Neo4jì— ë…¸ë“œì™€ ì—£ì§€ ì €ì¥ 
    neo4j_handler = Neo4jHandler()
    neo4j_handler.insert_nodes_and_edges(nodes, edges, brain_id)
    logging.info("Neo4jì— ë…¸ë“œì™€ ì—£ì§€ ì‚½ì… ì™„ë£Œ")

    # ë…¸ë“œ ì •ë³´ ì„ë² ë”© ë° ì €ì¥
    # - ê° ë…¸ë“œ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ì—¬ Qdrantì— upsert
    # embedding_service.update_index_and_get_embeddings(nodes, brain_id)
    # logging.info("ë²¡í„° DBì— ë…¸ë“œ ì„ë² ë”© ì €ì¥ ì™„ë£Œ")
    dur_ms = (time.perf_counter() - t0) * 1000
    logging.info("ì‹œê°„@@@@@@ %.3f s @@@@@@", dur_ms / 1000)

    return {
        "message": "í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ, ê·¸ë˜í”„(ë…¸ë“œì™€ ì—£ì§€)ê°€ ìƒì„±ë˜ì—ˆê³  ë²¡í„° DBì— ì„ë² ë”©ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "nodes": nodes,
        "edges": edges
    }


# LangChain Tool ì •ì˜ (LangChainì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
if LANGCHAIN_AVAILABLE:
    # Toolì„ ìœ„í•œ Pydantic ëª¨ë¸ ì •ì˜
    class NodeQualityInput(BaseModel):
        question: str = Field(description="ì‚¬ìš©ì ì§ˆë¬¸")
        node_names: list = Field(description="ê²€ìƒ‰ëœ ë…¸ë“œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸")
        node_scores: list = Field(description="ê° ë…¸ë“œì˜ ìœ ì‚¬ë„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸")
    
    class SchemaSufficiencyInput(BaseModel):
        question: str = Field(description="ì‚¬ìš©ì ì§ˆë¬¸")
        schema_summary: str = Field(description="ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ê²°ê³¼ ìš”ì•½")
    
    class SchemaOptimizationInput(BaseModel):
        question: str = Field(description="ì‚¬ìš©ì ì§ˆë¬¸")
        raw_schema_text: str = Field(description="ì›ë³¸ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸")
    
    def create_langchain_llm(ai_service, model: str, model_name: str):
        """LangChain LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        if model == "openai":
            return ChatOpenAI(model=model_name, temperature=0)
        elif model == "ollama":
            if ChatOllama is None:
                raise ValueError("ChatOllamaë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. langchain-communityê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return ChatOllama(model=model_name, temperature=0)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model}")
    
    def create_node_quality_tool(llm):
        """ë…¸ë“œ í’ˆì§ˆ í‰ê°€ Tool ìƒì„±"""
        def evaluate_nodes(question: str, node_names: list, node_scores: list) -> str:
            """ê²€ìƒ‰ëœ ë…¸ë“œë“¤ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  í•„í„°ë§"""
            node_info = [f"- {name} (ìœ ì‚¬ë„: {score:.2f})" for name, score in zip(node_names, node_scores)]
            nodes_text = "\n".join(node_info)
            
            prompt = (
                f"ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë…¸ë“œ ëª©ë¡ì…ë‹ˆë‹¤.\n\n"
                f"ì‚¬ìš©ì ì§ˆë¬¸: {question}\n\n"
                f"ê²€ìƒ‰ëœ ë…¸ë“œ ëª©ë¡:\n{nodes_text}\n\n"
                f"ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë…¸ë“œë§Œ ì„ íƒí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n"
                f'{{"filtered_node_names": ["ë…¸ë“œëª…1", "ë…¸ë“œëª…2", ...], "needs_more_search": true/false, "reason": "íŒë‹¨ ì´ìœ "}}'
            )
            
            try:
                response = llm.invoke(prompt)
                content = response.content if hasattr(response, 'content') else str(response)
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    return json_match.group()
                return json.dumps({"filtered_node_names": node_names, "needs_more_search": False, "reason": "íŒŒì‹± ì‹¤íŒ¨"})
            except Exception as e:
                logging.error(f"ë…¸ë“œ í’ˆì§ˆ í‰ê°€ Tool ì˜¤ë¥˜: {e}")
                return json.dumps({"filtered_node_names": node_names, "needs_more_search": False, "reason": f"ì˜¤ë¥˜: {str(e)}"})
        
        return StructuredTool.from_function(
            func=evaluate_nodes,
            name="evaluate_node_quality",
            description="ê²€ìƒ‰ëœ ë…¸ë“œë“¤ì˜ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤.",
            args_schema=NodeQualityInput
        )
    
    def create_schema_sufficiency_tool(llm):
        """ìŠ¤í‚¤ë§ˆ ì¶©ë¶„ì„± íŒë‹¨ Tool ìƒì„±"""
        def evaluate_schema(question: str, schema_summary: str) -> str:
            """ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ê²°ê³¼ì˜ ì¶©ë¶„ì„±ì„ íŒë‹¨"""
            prompt = (
                f"ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ê²°ê³¼ ìš”ì•½ì…ë‹ˆë‹¤.\n\n"
                f"ì‚¬ìš©ì ì§ˆë¬¸: {question}\n\n"
                f"ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ê²°ê³¼: {schema_summary}\n\n"
                f"ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆëŠ”ì§€ íŒë‹¨í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n"
                f'{{"is_sufficient": true/false, "needs_deep_search": true/false, "missing_info": "ë¶€ì¡±í•œ ì •ë³´", "reason": "íŒë‹¨ ì´ìœ "}}'
            )
            
            try:
                response = llm.invoke(prompt)
                content = response.content if hasattr(response, 'content') else str(response)
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    return json_match.group()
                return json.dumps({"is_sufficient": True, "needs_deep_search": False, "missing_info": "", "reason": "íŒŒì‹± ì‹¤íŒ¨"})
            except Exception as e:
                logging.error(f"ìŠ¤í‚¤ë§ˆ ì¶©ë¶„ì„± íŒë‹¨ Tool ì˜¤ë¥˜: {e}")
                return json.dumps({"is_sufficient": True, "needs_deep_search": False, "missing_info": "", "reason": f"ì˜¤ë¥˜: {str(e)}"})
        
        return StructuredTool.from_function(
            func=evaluate_schema,
            name="evaluate_schema_sufficiency",
            description="ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.",
            args_schema=SchemaSufficiencyInput
        )
    
    def create_schema_optimization_tool(llm):
        """ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ìµœì í™” Tool ìƒì„±"""
        def optimize_schema(question: str, raw_schema_text: str) -> str:
            """ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ë¥¼ ì§ˆë¬¸ì— ë§ê²Œ ìµœì í™”"""
            if not raw_schema_text or len(raw_schema_text.strip()) == 0:
                return raw_schema_text
            
            prompt = (
                f"ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n\n"
                f"ì‚¬ìš©ì ì§ˆë¬¸: {question}\n\n"
                f"ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸:\n{raw_schema_text}\n\n"
                f"ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë§Œ ë‚¨ê¸°ê³  ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì œê±°í•˜ì—¬ ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\n"
                f"ì›ë³¸ ìŠ¤í‚¤ë§ˆì˜ êµ¬ì¡°ì™€ í˜•ì‹ì€ ìœ ì§€í•˜ë˜, ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ë…¸ë“œë‚˜ ê´€ê³„ëŠ” ì œì™¸í•´ì£¼ì„¸ìš”."
            )
            
            try:
                response = llm.invoke(prompt)
                optimized_text = response.content if hasattr(response, 'content') else str(response)
                optimized_text = optimized_text.strip()
                
                if not optimized_text or len(optimized_text) < 10:
                    return raw_schema_text
                
                return optimized_text
            except Exception as e:
                logging.error(f"ìŠ¤í‚¤ë§ˆ ìµœì í™” Tool ì˜¤ë¥˜: {e}")
                return raw_schema_text
        
        return StructuredTool.from_function(
            func=optimize_schema,
            name="optimize_schema_text",
            description="ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ë¥¼ ì§ˆë¬¸ì— ë§ê²Œ ìµœì í™”í•˜ì—¬ ê´€ë ¨ ì •ë³´ë§Œ ë‚¨ê¹ë‹ˆë‹¤.",
            args_schema=SchemaOptimizationInput
        )


# ì˜¤ë¥˜ ë³µêµ¬ Agent í•¨ìˆ˜
def error_recovery_agent(ai_service, error_info: dict, step_name: str, context: dict) -> dict:
    """
    ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ì¸ì„ ë¶„ì„í•˜ê³  í•´ê²° ë°©ì•ˆì„ ì œì‹œí•˜ëŠ” AI Agent
    
    Args:
        ai_service: AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        error_info: ì˜¤ë¥˜ ì •ë³´ (error_type, error_message, step, context ë“±)
        step_name: ì˜¤ë¥˜ê°€ ë°œìƒí•œ ë‹¨ê³„ ì´ë¦„
        context: í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
    
    Returns:
        dict: {
            "recovery_action": str,  # ë³µêµ¬ ì•¡ì…˜ (retry, skip, modify, fallback ë“±)
            "modification": dict,  # ìˆ˜ì • ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
            "reason": str,  # ë³µêµ¬ ë°©ì•ˆ ì´ìœ 
            "retry_params": dict  # ì¬ì‹œë„ ì‹œ ì‚¬ìš©í•  íŒŒë¼ë¯¸í„°
        }
    """
    error_type = error_info.get("error_type", "Unknown")
    error_message = error_info.get("error_message", "")
    step = error_info.get("step", "")
    
    prompt = (
        f"ë‹¤ìŒì€ ì§ˆë¬¸-ë‹µë³€ íŒŒì´í”„ë¼ì¸ì—ì„œ ë°œìƒí•œ ì˜¤ë¥˜ì…ë‹ˆë‹¤.\n\n"
        f"ì˜¤ë¥˜ ë°œìƒ ë‹¨ê³„: {step_name}\n"
        f"ì˜¤ë¥˜ ìœ í˜•: {error_type}\n"
        f"ì˜¤ë¥˜ ë©”ì‹œì§€: {error_message}\n\n"
        f"í˜„ì¬ ì»¨í…ìŠ¤íŠ¸:\n"
        f"- ì§ˆë¬¸: {context.get('question', 'N/A')}\n"
        f"- ê²€ìƒ‰ëœ ë…¸ë“œ ìˆ˜: {context.get('node_count', 'N/A')}\n"
        f"- ìŠ¤í‚¤ë§ˆ ë…¸ë“œ ìˆ˜: {context.get('schema_node_count', 'N/A')}\n\n"
        f"ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n"
        f'{{"recovery_action": "retry|skip|modify|fallback", "modification": {{"key": "value"}}, "reason": "ë³µêµ¬ ë°©ì•ˆ ì´ìœ ", "retry_params": {{"param": "value"}}}}\n\n'
        f"ë³µêµ¬ ì•¡ì…˜ ì„¤ëª…:\n"
        f"- retry: ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¡œ ì¬ì‹œë„\n"
        f"- skip: í˜„ì¬ ë‹¨ê³„ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰\n"
        f"- modify: íŒŒë¼ë¯¸í„° ìˆ˜ì • í›„ ì¬ì‹œë„ (retry_paramsì— ìˆ˜ì • ì‚¬í•­ í¬í•¨)\n"
        f"- fallback: ëŒ€ì²´ ë°©ë²• ì‚¬ìš© (ì˜ˆ: ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€)\n\n"
        f"JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
    )
    
    try:
        response = ai_service.chat(prompt)
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            return {
                "recovery_action": result.get("recovery_action", "skip"),
                "modification": result.get("modification", {}),
                "reason": result.get("reason", ""),
                "retry_params": result.get("retry_params", {})
            }
        else:
            logging.warning("ì˜¤ë¥˜ ë³µêµ¬ Agent ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return {
                "recovery_action": "skip",
                "modification": {},
                "reason": "ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨",
                "retry_params": {}
            }
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë³µêµ¬ Agent ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return {
            "recovery_action": "skip",
            "modification": {},
            "reason": f"Agent ì˜¤ë¥˜: {str(e)}",
            "retry_params": {}
        }


def retry_with_recovery(max_retries=3):
    """
    ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ ë³µêµ¬ ë° ì¬ì‹œë„ë¥¼ ìœ„í•œ ë°ì½”ë ˆì´í„°
    
    Args:
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            ai_service = kwargs.get('ai_service') or (args[0] if args else None)
            question = kwargs.get('question') or (args[1] if len(args) > 1 else "")
            step_name = kwargs.get('step_name', func.__name__)
            
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    error_info = {
                        "error_type": type(e).__name__,
                        "error_message": str(e),
                        "step": step_name,
                        "attempt": attempt + 1
                    }
                    
                    context = {
                        "question": question,
                        "node_count": kwargs.get('node_count', 'N/A'),
                        "schema_node_count": kwargs.get('schema_node_count', 'N/A')
                    }
                    
                    logging.warning(f"âš ï¸  [{step_name}] ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                    
                    if attempt < max_retries - 1 and ai_service:
                        # ì˜¤ë¥˜ ë³µêµ¬ Agent ì‹¤í–‰
                        logging.info(f"ğŸ”§ ì˜¤ë¥˜ ë³µêµ¬ Agent ì‹¤í–‰ ì¤‘...")
                        recovery_result = error_recovery_agent(ai_service, error_info, step_name, context)
                        
                        recovery_action = recovery_result.get('recovery_action', 'skip')
                        logging.info(f"ğŸ”§ ë³µêµ¬ ë°©ì•ˆ: {recovery_action} - {recovery_result['reason']}")
                        
                        if recovery_action == "retry":
                            logging.info(f"ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                            continue
                        elif recovery_action == "modify":
                            # íŒŒë¼ë¯¸í„° ìˆ˜ì • í›„ ì¬ì‹œë„
                            retry_params = recovery_result.get("retry_params", {})
                            kwargs.update(retry_params)
                            logging.info(f"ğŸ”„ íŒŒë¼ë¯¸í„° ìˆ˜ì • í›„ ì¬ì‹œë„: {retry_params}")
                            continue
                        elif recovery_action == "skip":
                            logging.info(f"â­ï¸  í˜„ì¬ ë‹¨ê³„ ê±´ë„ˆë›°ê¸°")
                            return None
                        elif recovery_action == "fallback":
                            logging.info(f"ğŸ”„ ëŒ€ì²´ ë°©ë²• ì‚¬ìš©")
                            # fallback ë¡œì§ì€ ê° í•¨ìˆ˜ì—ì„œ êµ¬í˜„
                            return None
                    else:
                        # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
                        logging.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì˜¤ë¥˜: {str(e)}")
                        raise
            
            return None
        return wrapper
    return decorator


# AI Agent í•¨ìˆ˜ë“¤ (ì»¤ìŠ¤í…€ êµ¬í˜„ - LangChainì´ ì—†ì„ ë•Œ ì‚¬ìš©)
def evaluate_search_nodes_quality(ai_service, question: str, similar_nodes: list) -> dict:
    """
    ê²€ìƒ‰ëœ ë…¸ë“œë“¤ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ìµœì í™”í•˜ëŠ” AI Agent
    
    Args:
        ai_service: AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        question: ì‚¬ìš©ì ì§ˆë¬¸
        similar_nodes: ê²€ìƒ‰ëœ ìœ ì‚¬ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        dict: {
            "filtered_nodes": list,  # í•„í„°ë§ëœ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
            "needs_more_search": bool,  # ì¶”ê°€ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€
            "reason": str  # íŒë‹¨ ì´ìœ 
        }
    """
    if not similar_nodes:
        return {
            "filtered_nodes": [],
            "needs_more_search": False,
            "reason": "ê²€ìƒ‰ëœ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # ë…¸ë“œ ì´ë¦„ê³¼ ì ìˆ˜ ì •ë³´ ì¶”ì¶œ
    node_info = [f"- {node['name']} (ìœ ì‚¬ë„: {node['score']:.2f})" for node in similar_nodes]
    nodes_text = "\n".join(node_info)
    
    prompt = (
        f"ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë…¸ë“œ ëª©ë¡ì…ë‹ˆë‹¤.\n\n"
        f"ì‚¬ìš©ì ì§ˆë¬¸: {question}\n\n"
        f"ê²€ìƒ‰ëœ ë…¸ë“œ ëª©ë¡:\n{nodes_text}\n\n"
        f"ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n"
        f'{{"filtered_node_names": ["ë…¸ë“œëª…1", "ë…¸ë“œëª…2", ...], "needs_more_search": true/false, "reason": "íŒë‹¨ ì´ìœ "}}\n\n'
        f"íŒë‹¨ ê¸°ì¤€:\n"
        f"1. ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë…¸ë“œë§Œ í¬í•¨\n"
        f"2. ê´€ë ¨ì„±ì´ ë‚®ì€ ë…¸ë“œëŠ” ì œì™¸\n"
        f"3. ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ needs_more_searchë¥¼ trueë¡œ ì„¤ì •\n"
        f"4. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
    )
    
    try:
        response = ai_service.chat(prompt)
        # JSON íŒŒì‹± ì‹œë„
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
        else:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë…¸ë“œ ë°˜í™˜
            logging.warning("AI Agent ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ë…¸ë“œ ì‚¬ìš©")
            return {
                "filtered_nodes": similar_nodes,
                "needs_more_search": False,
                "reason": "ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
            }
        
        # í•„í„°ë§ëœ ë…¸ë“œ ì´ë¦„ìœ¼ë¡œ ì›ë³¸ ë…¸ë“œ í•„í„°ë§
        filtered_names = result.get("filtered_node_names", [])
        filtered_nodes = [node for node in similar_nodes if node["name"] in filtered_names]
        
        # í•„í„°ë§ ê²°ê³¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
        if not filtered_nodes:
            filtered_nodes = similar_nodes
        
        return {
            "filtered_nodes": filtered_nodes,
            "needs_more_search": result.get("needs_more_search", False),
            "reason": result.get("reason", "")
        }
    except Exception as e:
        logging.error(f"ê²€ìƒ‰ ë…¸ë“œ í’ˆì§ˆ í‰ê°€ Agent ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë…¸ë“œ ë°˜í™˜
        return {
            "filtered_nodes": similar_nodes,
            "needs_more_search": False,
            "reason": f"Agent ì˜¤ë¥˜: {str(e)}"
        }


def evaluate_schema_sufficiency(ai_service, question: str, schema_summary: str) -> dict:
    """
    ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ê²°ê³¼ì˜ ì¶©ë¶„ì„±ì„ íŒë‹¨í•˜ëŠ” AI Agent
    
    Args:
        ai_service: AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        question: ì‚¬ìš©ì ì§ˆë¬¸
        schema_summary: ìŠ¤í‚¤ë§ˆ ìš”ì•½ ì •ë³´ (ë…¸ë“œ ê°œìˆ˜, ê´€ê³„ ê°œìˆ˜ ë“±)
    
    Returns:
        dict: {
            "is_sufficient": bool,  # ì¶©ë¶„í•œì§€ ì—¬ë¶€
            "needs_deep_search": bool,  # ê¹Šì€ íƒìƒ‰ í•„ìš” ì—¬ë¶€
            "missing_info": str,  # ë¶€ì¡±í•œ ì •ë³´ ì„¤ëª…
            "reason": str  # íŒë‹¨ ì´ìœ 
        }
    """
    prompt = (
        f"ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ê²°ê³¼ ìš”ì•½ì…ë‹ˆë‹¤.\n\n"
        f"ì‚¬ìš©ì ì§ˆë¬¸: {question}\n\n"
        f"ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ê²°ê³¼: {schema_summary}\n\n"
        f"ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n"
        f'{{"is_sufficient": true/false, "needs_deep_search": true/false, "missing_info": "ë¶€ì¡±í•œ ì •ë³´", "reason": "íŒë‹¨ ì´ìœ "}}\n\n'
        f"íŒë‹¨ ê¸°ì¤€:\n"
        f"1. ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆëŠ”ì§€ íŒë‹¨\n"
        f"2. ë¶€ì¡±í•˜ë©´ needs_deep_searchë¥¼ trueë¡œ ì„¤ì •\n"
        f"3. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
    )
    
    try:
        response = ai_service.chat(prompt)
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
        else:
            logging.warning("AI Agent ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return {
                "is_sufficient": True,
                "needs_deep_search": False,
                "missing_info": "",
                "reason": "ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
            }
        
        return {
            "is_sufficient": result.get("is_sufficient", True),
            "needs_deep_search": result.get("needs_deep_search", False),
            "missing_info": result.get("missing_info", ""),
            "reason": result.get("reason", "")
        }
    except Exception as e:
        logging.error(f"ìŠ¤í‚¤ë§ˆ ì¶©ë¶„ì„± íŒë‹¨ Agent ì˜¤ë¥˜: {e}")
        return {
            "is_sufficient": True,
            "needs_deep_search": False,
            "missing_info": "",
            "reason": f"Agent ì˜¤ë¥˜: {str(e)}"
        }


def optimize_schema_text(ai_service, question: str, raw_schema_text: str) -> str:
    """
    ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ë¥¼ ì§ˆë¬¸ì— ë§ê²Œ ìµœì í™”í•˜ëŠ” AI Agent
    
    Args:
        ai_service: AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        question: ì‚¬ìš©ì ì§ˆë¬¸
        raw_schema_text: ì›ë³¸ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸
    
    Returns:
        str: ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸
    """
    if not raw_schema_text or len(raw_schema_text.strip()) == 0:
        return raw_schema_text
    
    prompt = (
        f"ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n\n"
        f"ì‚¬ìš©ì ì§ˆë¬¸: {question}\n\n"
        f"ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸:\n{raw_schema_text}\n\n"
        f"ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë§Œ ë‚¨ê¸°ê³  ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì œê±°í•˜ì—¬ ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\n"
        f"ì›ë³¸ ìŠ¤í‚¤ë§ˆì˜ êµ¬ì¡°ì™€ í˜•ì‹ì€ ìœ ì§€í•˜ë˜, ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ë…¸ë“œë‚˜ ê´€ê³„ëŠ” ì œì™¸í•´ì£¼ì„¸ìš”.\n"
        f"ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ë§Œ ì‘ë‹µí•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
    )
    
    try:
        optimized_text = ai_service.chat(prompt)
        optimized_text = optimized_text.strip()
        
        # ìµœì í™” ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
        if not optimized_text or len(optimized_text) < 10:
            logging.warning("ìµœì í™”ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ ì›ë³¸ ì‚¬ìš©")
            return raw_schema_text
        
        return optimized_text
    except Exception as e:
        logging.error(f"ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ìµœì í™” Agent ì˜¤ë¥˜: {e}")
        return raw_schema_text


@router.post("/answer",
    summary="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±",
    description="""
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ ì„ë² ë”©ì„ í†µí•´ ìœ ì‚¬í•œ ë…¸ë“œë¥¼ ì°¾ê³ , 
    í•´ë‹¹ ë…¸ë“œë“¤ì˜ 2ë‹¨ê³„ ê¹Šì´ ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œ í›„ LLMì„ ì´ìš©í•´ ìµœì¢… ë‹µë³€ ìƒì„±
    <br> Ollama ì‚¬ìš© â†’ (model: "ollama")  
    <br> GPT ì‚¬ìš© â†’ (model: "gpt")
    <br> ë”¥ì„œì¹˜ ì‚¬ìš© â†’ (use_deep_search : true) ë””í´íŠ¸ ê°’ falseë¡œ ì„¤ì •
    """,
    response_description="ìƒì„±ëœ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
    responses={
    500: {
        "description": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ (50001, 50002 ë“±)",
        "content": {
            "application/json": {
                "examples": {
                    "50001": ErrorExamples[50001]["content"]["application/json"],
                    "50002": ErrorExamples[50002]["content"]["application/json"]
                }
            }
        }
    }
}
)
async def answer_endpoint(request_data: AnswerRequest):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ ì„ë² ë”©ì„ í†µí•´ ìœ ì‚¬í•œ ë…¸ë“œë¥¼ ì°¾ê³ , 
    í•´ë‹¹ ë…¸ë“œë“¤ì˜ 2ë‹¨ê³„ ê¹Šì´ ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œ í›„ LLMì„ ì´ìš©í•´ ìµœì¢… ë‹µë³€ ìƒì„±
    <br> Ollama ì‚¬ìš© â†’ (model: "ollama")  
    <br> GPT ì‚¬ìš© â†’ (model: "gpt")
    """
    # ìš”ì²­ íŒŒë¼ë¯¸í„° ì–¸íŒ¨í‚¹ ë° ê¸°ë³¸ ê²€ì¦
    question = request_data.question
    session_id = request_data.session_id
    brain_id = str(request_data.brain_id)  # ë¬¸ìì—´ë¡œ ë³€í™˜
    model = request_data.model
    model_name = request_data.model_name
    use_deep_search = request_data.use_deep_search
    if not question:
        raise HTTPException(status_code=400, detail="question íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    if not brain_id:
        raise HTTPException(status_code=400, detail="brain_id íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì£¼ì…
    if model == "openai":
        ai_service = get_ai_service_GPT(model_name)
        logging.info("ğŸ“‹ [1] ì§ˆë¬¸ ìˆ˜ì‹  | ëª¨ë¸: %s (%s)", model_name, model)
    elif model == "ollama":
        ai_service = get_ai_service_Ollama(model_name)
        logging.info("ğŸ“‹ [1] ì§ˆë¬¸ ìˆ˜ì‹  | ëª¨ë¸: %s (%s)", model_name, model)
    else:
        logging.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: %s", model)
        raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model}")
    
    logging.info("ğŸ’¬ ì§ˆë¬¸: %s", question[:100] + "..." if len(question) > 100 else question)
    
    try:
        # SQLite í•¸ë“¤ëŸ¬: ì†ŒìŠ¤ ë©”íƒ€ë°ì´í„°(title ë“±) ì¡°íšŒì™€ ì±„íŒ… ë¡œê·¸ ì €ì¥ì— ì‚¬ìš©
        db_handler = SQLiteHandler()
        
        # Step 1: ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if not embedding_service.is_index_ready(brain_id):
            embedding_service.initialize_collection(brain_id)
        
        # Step 2: ì§ˆë¬¸ ì„ë² ë”© ê³„ì‚°
        question_embedding = embedding_service.encode_text(question)
        
        # Step 3: ì„ë² ë”©ì„ í†µí•´ ìœ ì‚¬í•œ ë…¸ë“œ ê²€ìƒ‰
        logging.info("ğŸ” [2] ìœ ì‚¬ ë…¸ë“œ ê²€ìƒ‰ ì¤‘...")
        similar_nodes,Q = embedding_service.search_similar_nodes(embedding=question_embedding, brain_id=brain_id)
        if not similar_nodes:
            # ê´€ë ¨ ë…¸ë“œê°€ ì—†ì„ ë•Œ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            logging.info("âš ï¸  [3] ê´€ë ¨ ë…¸ë“œ ì—†ìŒ â†’ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
            general_prompt = (
                f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. "
                f"ì—…ë¡œë“œëœ ì†ŒìŠ¤ íŒŒì¼ì„ ì°¸ê³ í•˜ì§€ ë§ê³ , ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
                f"ì§ˆë¬¸: {question}\n\n"
                f"ë‹µë³€:"
            )
            final_answer = ai_service.chat(general_prompt)
            final_answer = final_answer.strip()
            
            # ì¼ë°˜ ì§€ì‹ ë‹µë³€ ì €ì¥
            chat_id = db_handler.save_chat(session_id, True, final_answer, [], 0.0)
            logging.info("âœ… [4] ì™„ë£Œ | ì¼ë°˜ ì§€ì‹ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            return {
                "answer": final_answer,
                "referenced_nodes": [],
                "chat_id": chat_id,
                "accuracy": 0.0
            }
        
        # Step 3-1: [AI Agent] ê²€ìƒ‰ëœ ë…¸ë“œ í’ˆì§ˆ í‰ê°€ ë° ìµœì í™”
        initial_node_count = len(similar_nodes)
        logging.info("ğŸ¤– [3] AI Agent: ë…¸ë“œ í’ˆì§ˆ í‰ê°€ ì¤‘... (ê²€ìƒ‰ëœ ë…¸ë“œ: %dê°œ)", initial_node_count)
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if LANGCHAIN_AVAILABLE:
                    try:
                        llm = create_langchain_llm(ai_service, model, model_name)
                        node_quality_tool = create_node_quality_tool(llm)
                        node_names = [node["name"] for node in similar_nodes]
                        node_scores = [node["score"] for node in similar_nodes]
                        result_json = node_quality_tool.invoke({
                            "question": question,
                            "node_names": node_names,
                            "node_scores": node_scores
                        })
                        result = json.loads(result_json)
                        filtered_names = result.get("filtered_node_names", [])
                        filtered_nodes = [node for node in similar_nodes if node["name"] in filtered_names]
                        if not filtered_nodes:
                            filtered_nodes = similar_nodes
                        similar_nodes = filtered_nodes
                    except Exception as e:
                        node_quality_result = evaluate_search_nodes_quality(ai_service, question, similar_nodes)
                        similar_nodes = node_quality_result["filtered_nodes"]
                else:
                    node_quality_result = evaluate_search_nodes_quality(ai_service, question, similar_nodes)
                    similar_nodes = node_quality_result["filtered_nodes"]
                break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
            except Exception as e:
                error_info = {
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "step": "ë…¸ë“œ í’ˆì§ˆ í‰ê°€",
                    "attempt": attempt + 1
                }
                context = {
                    "question": question,
                    "node_count": len(similar_nodes),
                    "schema_node_count": "N/A"
                }
                
                logging.warning(f"âš ï¸  [ë…¸ë“œ í’ˆì§ˆ í‰ê°€] ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                
                if attempt < max_retries - 1:
                    logging.info("ğŸ”§ ì˜¤ë¥˜ ë³µêµ¬ Agent ì‹¤í–‰ ì¤‘...")
                    recovery_result = error_recovery_agent(ai_service, error_info, "ë…¸ë“œ í’ˆì§ˆ í‰ê°€", context)
                    recovery_action = recovery_result.get('recovery_action', 'skip')
                    logging.info(f"ğŸ”§ ë³µêµ¬ ë°©ì•ˆ: {recovery_action} - {recovery_result['reason']}")
                    
                    if recovery_action == "retry":
                        logging.info("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                        continue
                    elif recovery_action == "skip":
                        logging.info("â­ï¸  í˜„ì¬ ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (ì›ë³¸ ë…¸ë“œ ì‚¬ìš©)")
                        break
                    elif recovery_action == "fallback":
                        logging.info("ğŸ”„ ëŒ€ì²´ ë°©ë²• ì‚¬ìš© (ì›ë³¸ ë…¸ë“œ ì‚¬ìš©)")
                        break
                else:
                    logging.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì›ë³¸ ë…¸ë“œ ì‚¬ìš©")
                    break
        
        filtered_node_count = len(similar_nodes)
        if initial_node_count != filtered_node_count:
            logging.info("âœ“ ìµœì í™” ì™„ë£Œ: %dê°œ â†’ %dê°œ (ê´€ë ¨ì„± ë‚®ì€ ë…¸ë“œ %dê°œ ì œê±°)", 
                        initial_node_count, filtered_node_count, initial_node_count - filtered_node_count)
        else:
            logging.info("âœ“ ìµœì í™” ì™„ë£Œ: ëª¨ë“  ë…¸ë“œ ìœ ì§€ (%dê°œ)", filtered_node_count)
        similar_node_names = [node["name"] for node in similar_nodes]
        
        # Step 4: ìœ ì‚¬í•œ ë…¸ë“œë“¤ì˜ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
        logging.info("ğŸ—ºï¸  [4] ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì¤‘...")
        neo4j_handler = Neo4jHandler()
        
        max_retries = 3
        result = None
        for attempt in range(max_retries):
            try:
                if(use_deep_search):
                    result = neo4j_handler.query_schema_by_node_names_deepSearch(similar_node_names, brain_id)
                else:
                    result = neo4j_handler.query_schema_by_node_names(similar_node_names, brain_id)
                break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
            except Exception as e:
                error_info = {
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "step": "ìŠ¤í‚¤ë§ˆ ì¡°íšŒ",
                    "attempt": attempt + 1
                }
                context = {
                    "question": question,
                    "node_count": len(similar_nodes),
                    "schema_node_count": "N/A"
                }
                
                logging.warning(f"âš ï¸  [ìŠ¤í‚¤ë§ˆ ì¡°íšŒ] ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                
                if attempt < max_retries - 1:
                    logging.info("ğŸ”§ ì˜¤ë¥˜ ë³µêµ¬ Agent ì‹¤í–‰ ì¤‘...")
                    recovery_result = error_recovery_agent(ai_service, error_info, "ìŠ¤í‚¤ë§ˆ ì¡°íšŒ", context)
                    recovery_action = recovery_result.get('recovery_action', 'fallback')
                    logging.info(f"ğŸ”§ ë³µêµ¬ ë°©ì•ˆ: {recovery_action} - {recovery_result['reason']}")
                    
                    if recovery_action == "retry":
                        logging.info("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                        continue
                    elif recovery_action == "modify":
                        # íŒŒë¼ë¯¸í„° ìˆ˜ì • (ì˜ˆ: deep_search í† ê¸€)
                        retry_params = recovery_result.get("retry_params", {})
                        if retry_params.get("use_deep_search") is not None:
                            use_deep_search = retry_params["use_deep_search"]
                            logging.info(f"ğŸ”„ íŒŒë¼ë¯¸í„° ìˆ˜ì •: use_deep_search={use_deep_search}")
                        continue
                    elif recovery_action == "fallback":
                        logging.info("ğŸ”„ ëŒ€ì²´ ë°©ë²• ì‚¬ìš© (ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€)")
                        result = None
                        break
                else:
                    logging.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
                    result = None
                    break

        if not result:
            logging.info("âš ï¸  [5] ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ê²°ê³¼ ì—†ìŒ â†’ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
            general_prompt = (
                f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. "
                f"ì—…ë¡œë“œëœ ì†ŒìŠ¤ íŒŒì¼ì„ ì°¸ê³ í•˜ì§€ ë§ê³ , ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
                f"ì§ˆë¬¸: {question}\n\n"
                f"ë‹µë³€:"
            )
            final_answer = ai_service.chat(general_prompt)
            final_answer = final_answer.strip()
            
            # ì¼ë°˜ ì§€ì‹ ë‹µë³€ ì €ì¥
            chat_id = db_handler.save_chat(session_id, True, final_answer, [], 0.0)
            logging.info("âœ… [6] ì™„ë£Œ | ì¼ë°˜ ì§€ì‹ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            return {
                "answer": final_answer,
                "referenced_nodes": [],
                "chat_id": chat_id,
                "accuracy": 0.0
            }
            
        # ê²°ê³¼ë¥¼ ì¦‰ì‹œ ì²˜ë¦¬
        nodes_result = result.get("nodes", [])
        related_nodes_result = result.get("relatedNodes", [])
        relationships_result = result.get("relationships", [])
        
        logging.info("âœ“ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì™„ë£Œ: ë…¸ë“œ %dê°œ, ê´€ê³„ %dê°œ", len(nodes_result), len(relationships_result))
        
        # Step 4-1: [AI Agent] ìŠ¤í‚¤ë§ˆ ì¶©ë¶„ì„± íŒë‹¨
        initial_nodes_count = len(nodes_result)
        initial_relationships_count = len(relationships_result)
        schema_summary = f"ë…¸ë“œ {len(nodes_result)}ê°œ, ê´€ë ¨ ë…¸ë“œ {len(related_nodes_result)}ê°œ, ê´€ê³„ {len(relationships_result)}ê°œ"
        logging.info("ğŸ¤– [5] AI Agent: ìŠ¤í‚¤ë§ˆ ì¶©ë¶„ì„± íŒë‹¨ ì¤‘... (í˜„ì¬: ë…¸ë“œ %dê°œ, ê´€ê³„ %dê°œ)", 
                    initial_nodes_count, initial_relationships_count)
        
        max_retries = 3
        schema_sufficiency_result = {"is_sufficient": True, "needs_deep_search": False}
        for attempt in range(max_retries):
            try:
                if LANGCHAIN_AVAILABLE:
                    try:
                        llm = create_langchain_llm(ai_service, model, model_name)
                        schema_sufficiency_tool = create_schema_sufficiency_tool(llm)
                        result_json = schema_sufficiency_tool.invoke({
                            "question": question,
                            "schema_summary": schema_summary
                        })
                        schema_sufficiency_result = json.loads(result_json)
                    except Exception as e:
                        schema_sufficiency_result = evaluate_schema_sufficiency(ai_service, question, schema_summary)
                else:
                    schema_sufficiency_result = evaluate_schema_sufficiency(ai_service, question, schema_summary)
                break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
            except Exception as e:
                error_info = {
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "step": "ìŠ¤í‚¤ë§ˆ ì¶©ë¶„ì„± íŒë‹¨",
                    "attempt": attempt + 1
                }
                context = {
                    "question": question,
                    "node_count": len(similar_nodes),
                    "schema_node_count": len(nodes_result)
                }
                
                logging.warning(f"âš ï¸  [ìŠ¤í‚¤ë§ˆ ì¶©ë¶„ì„± íŒë‹¨] ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                
                if attempt < max_retries - 1:
                    logging.info("ğŸ”§ ì˜¤ë¥˜ ë³µêµ¬ Agent ì‹¤í–‰ ì¤‘...")
                    recovery_result = error_recovery_agent(ai_service, error_info, "ìŠ¤í‚¤ë§ˆ ì¶©ë¶„ì„± íŒë‹¨", context)
                    recovery_action = recovery_result.get('recovery_action', 'skip')
                    logging.info(f"ğŸ”§ ë³µêµ¬ ë°©ì•ˆ: {recovery_action} - {recovery_result['reason']}")
                    
                    if recovery_action == "retry":
                        logging.info("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                        continue
                    elif recovery_action == "skip":
                        logging.info("â­ï¸  í˜„ì¬ ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (ê¸°ë³¸ê°’ ì‚¬ìš©)")
                        schema_sufficiency_result = {"is_sufficient": True, "needs_deep_search": False}
                        break
                else:
                    logging.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ê¸°ë³¸ê°’ ì‚¬ìš©")
                    schema_sufficiency_result = {"is_sufficient": True, "needs_deep_search": False}
                    break
        
        # ì¶©ë¶„í•˜ì§€ ì•Šê³  ê¹Šì€ íƒìƒ‰ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ deep search ì‹œë„
        if not schema_sufficiency_result.get("is_sufficient", True) and schema_sufficiency_result.get("needs_deep_search", False) and not use_deep_search:
            logging.info("ğŸ” ì •ë³´ ë¶€ì¡± ê°ì§€ â†’ ê¹Šì€ íƒìƒ‰ ì‹¤í–‰...")
            result = neo4j_handler.query_schema_by_node_names_deepSearch(similar_node_names, brain_id)
            if result:
                nodes_result = result.get("nodes", [])
                related_nodes_result = result.get("relatedNodes", [])
                relationships_result = result.get("relationships", [])
                final_nodes_count = len(nodes_result)
                final_relationships_count = len(relationships_result)
                logging.info("âœ“ ê¹Šì€ íƒìƒ‰ ì™„ë£Œ: ë…¸ë“œ %dê°œ â†’ %dê°œ (+%dê°œ), ê´€ê³„ %dê°œ â†’ %dê°œ (+%dê°œ)", 
                            initial_nodes_count, final_nodes_count, final_nodes_count - initial_nodes_count,
                            initial_relationships_count, final_relationships_count, final_relationships_count - initial_relationships_count)
            else:
                logging.info("âœ“ ê¹Šì€ íƒìƒ‰ ì™„ë£Œ: ì¶”ê°€ ì •ë³´ ì—†ìŒ")
        else:
            if schema_sufficiency_result.get("is_sufficient", True):
                logging.info("âœ“ ì¶©ë¶„ì„± íŒë‹¨: í˜„ì¬ ìŠ¤í‚¤ë§ˆë¡œ ë‹µë³€ ê°€ëŠ¥")
            else:
                logging.info("âœ“ ì¶©ë¶„ì„± íŒë‹¨: í˜„ì¬ ìŠ¤í‚¤ë§ˆë¡œ ì§„í–‰ (ê¹Šì€ íƒìƒ‰ ë¶ˆí•„ìš”)")
        
        # Step 5: ìŠ¤í‚¤ë§ˆ ê°„ê²°í™” ë° í…ìŠ¤íŠ¸ êµ¬ì„±
        logging.info("ğŸ“ [6] ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        raw_schema_text = ai_service.generate_schema_text(nodes_result, related_nodes_result, relationships_result)
        initial_schema_length = len(raw_schema_text)
        logging.info("  â†’ ì›ë³¸ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸: %dì", initial_schema_length)
        
        # Step 5-1: [AI Agent] ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ìµœì í™”
        logging.info("ğŸ¤– [7] AI Agent: ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ìµœì í™” ì¤‘...")
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if LANGCHAIN_AVAILABLE:
                    try:
                        llm = create_langchain_llm(ai_service, model, model_name)
                        schema_optimization_tool = create_schema_optimization_tool(llm)
                        optimized_schema_text = schema_optimization_tool.invoke({
                            "question": question,
                            "raw_schema_text": raw_schema_text
                        })
                        if optimized_schema_text != raw_schema_text:
                            raw_schema_text = optimized_schema_text
                            optimized_length = len(raw_schema_text)
                            reduction = initial_schema_length - optimized_length
                            reduction_rate = (reduction / initial_schema_length * 100) if initial_schema_length > 0 else 0
                            logging.info("âœ“ ìµœì í™” ì™„ë£Œ: %dì â†’ %dì (ë¶ˆí•„ìš”í•œ ì •ë³´ %dì ì œê±°, %.1f%% ê°ì†Œ)", 
                                        initial_schema_length, optimized_length, reduction, reduction_rate)
                        else:
                            logging.info("âœ“ ìµœì í™” ì™„ë£Œ: ë³€ê²½ ì—†ìŒ (ì´ë¯¸ ìµœì í™”ëœ ìƒíƒœ)")
                    except Exception as e:
                        optimized_schema_text = optimize_schema_text(ai_service, question, raw_schema_text)
                        if optimized_schema_text != raw_schema_text:
                            raw_schema_text = optimized_schema_text
                            optimized_length = len(raw_schema_text)
                            reduction = initial_schema_length - optimized_length
                            reduction_rate = (reduction / initial_schema_length * 100) if initial_schema_length > 0 else 0
                            logging.info("âœ“ ìµœì í™” ì™„ë£Œ: %dì â†’ %dì (ë¶ˆí•„ìš”í•œ ì •ë³´ %dì ì œê±°, %.1f%% ê°ì†Œ)", 
                                        initial_schema_length, optimized_length, reduction, reduction_rate)
                        else:
                            logging.info("âœ“ ìµœì í™” ì™„ë£Œ: ë³€ê²½ ì—†ìŒ")
                else:
                    optimized_schema_text = optimize_schema_text(ai_service, question, raw_schema_text)
                    if optimized_schema_text != raw_schema_text:
                        raw_schema_text = optimized_schema_text
                        optimized_length = len(raw_schema_text)
                        reduction = initial_schema_length - optimized_length
                        reduction_rate = (reduction / initial_schema_length * 100) if initial_schema_length > 0 else 0
                        logging.info("âœ“ ìµœì í™” ì™„ë£Œ: %dì â†’ %dì (ë¶ˆí•„ìš”í•œ ì •ë³´ %dì ì œê±°, %.1f%% ê°ì†Œ)", 
                                    initial_schema_length, optimized_length, reduction, reduction_rate)
                    else:
                        logging.info("âœ“ ìµœì í™” ì™„ë£Œ: ë³€ê²½ ì—†ìŒ")
                break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
            except Exception as e:
                error_info = {
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "step": "ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ìµœì í™”",
                    "attempt": attempt + 1
                }
                context = {
                    "question": question,
                    "node_count": len(similar_nodes),
                    "schema_node_count": len(nodes_result)
                }
                
                logging.warning(f"âš ï¸  [ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ìµœì í™”] ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                
                if attempt < max_retries - 1:
                    logging.info("ğŸ”§ ì˜¤ë¥˜ ë³µêµ¬ Agent ì‹¤í–‰ ì¤‘...")
                    recovery_result = error_recovery_agent(ai_service, error_info, "ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ìµœì í™”", context)
                    recovery_action = recovery_result.get('recovery_action', 'skip')
                    logging.info(f"ğŸ”§ ë³µêµ¬ ë°©ì•ˆ: {recovery_action} - {recovery_result['reason']}")
                    
                    if recovery_action == "retry":
                        logging.info("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                        continue
                    elif recovery_action == "skip":
                        logging.info("â­ï¸  í˜„ì¬ ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (ì›ë³¸ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ì‚¬ìš©)")
                        break
                else:
                    logging.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì›ë³¸ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ì‚¬ìš©")
                    break
        
        # Step 6: LLMì„ ì‚¬ìš©í•´ ìµœì¢… ë‹µë³€ ìƒì„±
        logging.info("ğŸ’¡ [8] LLM ë‹µë³€ ìƒì„± ì¤‘...")
        
        max_retries = 3
        final_answer = None
        for attempt in range(max_retries):
            try:
                final_answer = ai_service.generate_answer(raw_schema_text, question)
                final_answer = final_answer.strip()
                break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
            except Exception as e:
                error_info = {
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "step": "LLM ë‹µë³€ ìƒì„±",
                    "attempt": attempt + 1
                }
                context = {
                    "question": question,
                    "node_count": len(similar_nodes),
                    "schema_node_count": len(nodes_result)
                }
                
                logging.warning(f"âš ï¸  [LLM ë‹µë³€ ìƒì„±] ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                
                if attempt < max_retries - 1:
                    logging.info("ğŸ”§ ì˜¤ë¥˜ ë³µêµ¬ Agent ì‹¤í–‰ ì¤‘...")
                    recovery_result = error_recovery_agent(ai_service, error_info, "LLM ë‹µë³€ ìƒì„±", context)
                    recovery_action = recovery_result.get('recovery_action', 'fallback')
                    logging.info(f"ğŸ”§ ë³µêµ¬ ë°©ì•ˆ: {recovery_action} - {recovery_result['reason']}")
                    
                    if recovery_action == "retry":
                        logging.info("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                        continue
                    elif recovery_action == "modify":
                        # íŒŒë¼ë¯¸í„° ìˆ˜ì • (ì˜ˆ: ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ë‹¨ìˆœí™”)
                        retry_params = recovery_result.get("retry_params", {})
                        if retry_params.get("simplify_schema"):
                            # ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœí™”
                            raw_schema_text = raw_schema_text[:1000] + "..." if len(raw_schema_text) > 1000 else raw_schema_text
                            logging.info("ğŸ”„ ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ë‹¨ìˆœí™” í›„ ì¬ì‹œë„")
                        continue
                    elif recovery_action == "fallback":
                        logging.info("ğŸ”„ ëŒ€ì²´ ë°©ë²• ì‚¬ìš© (ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€)")
                        final_answer = None
                        break
                else:
                    logging.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
                    final_answer = None
                    break
        
        # ì¼ë°˜ ì§€ì‹ ë‹µë³€ ì—¬ë¶€ í”Œë˜ê·¸
        is_general_knowledge_answer = False
        
        # ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” "ì§€ì‹ê·¸ë˜í”„ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if not final_answer or "ì§€ì‹ê·¸ë˜í”„ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" in final_answer or "ì§€ì‹ê·¸ë˜í”„ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤." in final_answer:
            logging.info("âš ï¸  ì§€ì‹ê·¸ë˜í”„ ì •ë³´ ì—†ìŒ â†’ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ì¬ìƒì„±")
            general_prompt = (
                f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. "
                f"ì—…ë¡œë“œëœ ì†ŒìŠ¤ íŒŒì¼ì„ ì°¸ê³ í•˜ì§€ ë§ê³ , ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
                f"ì§ˆë¬¸: {question}\n\n"
                f"ë‹µë³€:"
            )
            final_answer = ai_service.chat(general_prompt)
            final_answer = final_answer.strip()
            referenced_nodes = []
            Q = 0.0
            is_general_knowledge_answer = True
        else:
            # referenced_nodes = ai_service.extract_referenced_nodes(final_answer)
            referenced_nodes = ai_service.generate_referenced_nodes(final_answer,brain_id) #ì¶œì²˜ ë…¸ë“œ ë°˜í™˜ í•¨ìˆ˜
        
        # referenced_nodes ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ final_answer ë’¤ì— ì¶”ê°€
        if referenced_nodes:
            nodes_text = "\n\n[ì°¸ê³ ëœ ë…¸ë“œ ëª©ë¡]\n" + "\n".join(f"- {node}" for node in referenced_nodes)
            final_answer += nodes_text
        
        # ì¼ë°˜ ì§€ì‹ ë‹µë³€ì¸ì§€ í™•ì¸
        if is_general_knowledge_answer:
            # ì¼ë°˜ ì§€ì‹ ë‹µë³€ì¸ ê²½ìš° í›„ì²˜ë¦¬ ìƒëµ
            enriched = []
            accuracy = 0.0
            logging.info("âœ… [9] ì™„ë£Œ | ì¼ë°˜ ì§€ì‹ ë‹µë³€ ìƒì„±")
        else:
            # ê°„ë‹¨ ì •í™•ë„ ì‚°ì¶œ: ë‹µë³€/ì°¸ê³ ë…¸ë“œ/ë¸Œë ˆì¸/ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§€í‘œ
            logging.info("ğŸ“Š [9] í›„ì²˜ë¦¬: ì°¸ì¡° ë…¸ë“œ ì¶”ì¶œ ë° ì •í™•ë„ ê³„ì‚° ì¤‘...")
            accuracy = compute_accuracy(final_answer,referenced_nodes,brain_id,Q,raw_schema_text)
            # nodeì˜ ì¶œì²˜ ì†ŒìŠ¤ idë“¤ ê°€ì ¸ì˜¤ê¸°
            node_to_ids = neo4j_handler.get_descriptions_bulk(referenced_nodes, brain_id)
            # ëª¨ë“  source_id ì§‘í•© ìˆ˜ì§‘
            all_ids = sorted({sid for ids in node_to_ids.values() for sid in ids})
            # SQLite batch ì¡°íšŒë¡œ idâ†’title ë§¤í•‘
            id_to_title = db_handler.get_titles_by_ids(all_ids)
                   
            # ìµœì¢… êµ¬ì¡°í™”
            enriched = []
            for node in referenced_nodes:
                # ì¤‘ë³µ ì œê±°ëœ source_id ë¦¬ìŠ¤íŠ¸
                unique_sids = list(dict.fromkeys(node_to_ids.get(node, [])))
                sources = []
                for sid in unique_sids:
                    if sid not in id_to_title:
                        continue
                    # Neo4j ì—ì„œ ì´ (node, sid) ì¡°í•©ì˜ original_sentences ê°€ì ¸ì˜¤ê¸°
                    orig_sents = neo4j_handler.get_original_sentences(node, sid, brain_id)

                    sources.append({
                        "id": str(sid),
                        "title": id_to_title[sid],
                        "original_sentences": orig_sents  # ì—¬ê¸°ì— ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë“¤ì–´ê°
                    })

                enriched.append({
                        "name": node,
                        "source_ids": sources
                })

        # AI ë‹µë³€ ì €ì¥ ë° chat_id íšë“
        chat_id = db_handler.save_chat(session_id, True, final_answer, enriched, accuracy)
        
        logging.info("âœ… [10] ì™„ë£Œ | ë‹µë³€ ìƒì„± ì™„ë£Œ (ì •í™•ë„: %.2f)", accuracy)

        return {
            "answer": final_answer,
            "referenced_nodes": enriched,
            "chat_id": chat_id,
            "accuracy": accuracy
        }
    except Exception as e:
        logging.error("answer ì˜¤ë¥˜: %s", str(e))
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/getSourceIds",
    summary="ë…¸ë“œì˜ ëª¨ë“  source_idì™€ ì œëª©ì„ ì¡°íšŒ",
    description="íŠ¹ì • ë…¸ë“œì˜ descriptions ë°°ì—´ì—ì„œ ëª¨ë“  source_idë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.",
    response_description="source_idì™€ titleì„ í¬í•¨í•˜ëŠ” ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
    responses={
    500: ErrorExamples[50002]
    }
)
@router.get("/getNodeDescriptions",
    summary="ë…¸ë“œì˜ descriptions ì¡°íšŒ",
    description="íŠ¹ì • ë…¸ë“œì˜ descriptions ë°°ì—´ì„ ì¡°íšŒí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.",
    response_description="ë…¸ë“œì˜ descriptions ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
    responses={
        404: {
            "description": "ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ",
            "content": {
                "application/json": {
                    "example": {
                        "detail": "ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    }
                }
            }
        },
        500: ErrorExamples[50002]
    }
)
async def get_node_descriptions_endpoint(node_name: str, brain_id: str):
    """
    íŠ¹ì • ë…¸ë“œì˜ descriptions ë°°ì—´ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Args:
        node_name (str): ì¡°íšŒí•  ë…¸ë“œì˜ ì´ë¦„
        brain_id (str): ë¸Œë ˆì¸ ID
    
    Returns:
        dict: descriptions ì •ë³´
            - node_name: ë…¸ë“œ ì´ë¦„
            - brain_id: ë¸Œë ˆì¸ ID
            - descriptions: description ê°ì²´ë“¤ì˜ ë°°ì—´
                - ê° ê°ì²´ëŠ” description, source_id ë“±ì„ í¬í•¨
            - descriptions_count: descriptions ë°°ì—´ì˜ ê¸¸ì´
    
    Example Response:
        {
            "node_name": "ì¸ê³µì§€ëŠ¥",
            "brain_id": "brain123",
            "descriptions": [
                {
                    "description": "ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ",
                    "source_id": "101"
                },
                {
                    "description": "ê¸°ê³„í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ì„ í¬í•¨í•˜ëŠ” ê´‘ë²”ìœ„í•œ ë¶„ì•¼",
                    "source_id": "102"
                }
            ],
            "descriptions_count": 2
        }
    """
    logging.info(f"getNodeDescriptions ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ - node_name: {node_name}, brain_id: {brain_id}")
    
    # íŒŒë¼ë¯¸í„° ê²€ì¦
    if not node_name:
        raise HTTPException(status_code=400, detail="node_name íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    if not brain_id:
        raise HTTPException(status_code=400, detail="brain_id íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    try:
        # Neo4j í•¸ë“¤ëŸ¬ ìƒì„±
        neo4j_handler = Neo4jHandler()
        logging.info("Neo4j í•¸ë“¤ëŸ¬ ìƒì„±ë¨")
        
        # Neo4jì—ì„œ ë…¸ë“œì˜ descriptions ë°°ì—´ ì¡°íšŒ
        descriptions = neo4j_handler.get_node_descriptions(node_name, brain_id)
        
        # ë…¸ë“œê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ descriptionsê°€ ì—†ëŠ” ê²½ìš°
        if descriptions is None:
            logging.warning(f"ë…¸ë“œ '{node_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (brain_id: {brain_id})")
            raise HTTPException(status_code=404, detail="ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # descriptionsê°€ ë¹ˆ ë°°ì—´ì¸ ê²½ìš°ë„ ì •ìƒ ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬
        logging.info(f"ì¡°íšŒëœ descriptions ê°œìˆ˜: {len(descriptions)}")
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        response_data = {
            "node_name": node_name,
            "brain_id": brain_id,
            "descriptions": descriptions,
            "descriptions_count": len(descriptions)
        }
        
        # ê° descriptionì˜ source_id ëª©ë¡ë„ ì¶”ê°€ (ì„ íƒì )
        source_ids = list(set(desc.get("source_id") for desc in descriptions if "source_id" in desc))
        if source_ids:
            response_data["unique_source_ids"] = source_ids
            response_data["unique_source_count"] = len(source_ids)
        
        return response_data
        
    except HTTPException:
        # HTTP ì˜ˆì™¸ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
        raise
    except Exception as e:
        logging.error("descriptions ì¡°íšŒ ì˜¤ë¥˜: %s", str(e))
        raise HTTPException(status_code=500, detail=f"descriptions ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
async def get_source_ids(node_name: str, brain_id: str):
    """
    ë…¸ë“œì˜ ëª¨ë“  source_idì™€ ì œëª©ì„ ë°˜í™˜í•©ë‹ˆë‹¤:
    
    - **node_name**: ì¡°íšŒí•  ë…¸ë“œì˜ ì´ë¦„
    - **brain_id**: ë¸Œë ˆì¸ ID
    
    ë°˜í™˜ê°’:
    - **sources**: source_idì™€ titleì„ í¬í•¨í•˜ëŠ” ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    # íŒŒë¼ë¯¸í„° ê¸°ë¡: ìš´ì˜ ë””ë²„ê¹… ì‹œ í™œìš©
    logging.info(f"getSourceIds ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ - node_name: {node_name}, brain_id: {brain_id}")
    try:
        neo4j_handler = Neo4jHandler()
        db = SQLiteHandler()
        logging.info("Neo4j í•¸ë“¤ëŸ¬ ìƒì„±ë¨")
        
        # Neo4jì—ì„œ ë…¸ë“œì˜ descriptions ë°°ì—´ ì¡°íšŒ
        descriptions = neo4j_handler.get_node_descriptions(node_name, brain_id)
        if not descriptions:
            return {"sources": []}
            
        # descriptions ë°°ì—´ì—ì„œ ëª¨ë“  source_id ì¶”ì¶œ
        # - JSON í•­ëª© ê°„ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš©
        seen_ids = set()
        sources = []
        
        for desc in descriptions:
            if "source_id" in desc:
                source_id = desc["source_id"]
                if source_id not in seen_ids:
                    seen_ids.add(source_id)
                    
                    # PDFì™€ TextFile í…Œì´ë¸”ì—ì„œ ëª¨ë‘ ì¡°íšŒ
                    pdf = db.get_pdf(int(source_id))
                    textfile = db.get_textfile(int(source_id))
                    memo = db.get_memo(int(source_id))
                    md = db.get_mdfile(int(source_id))
                    
                    title = None
                    if pdf:
                        title = pdf['pdf_title']
                    elif textfile:
                        title = textfile['txt_title']
                    elif memo:
                        title = memo['memo_title']
                    elif md:
                        title = md['md_title']
                    
                    if title:
                        sources.append({
                            "id": source_id,
                            "title": title
                        })
        
        logging.info(f"ì¶”ì¶œëœ sources: {sources}")
        return {"sources": sources}
        
    except Exception as e:
        logging.error("source_id ì¡°íšŒ ì˜¤ë¥˜: %s", str(e))
        raise HTTPException(status_code=500, detail=f"source_id ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")



@router.get("/getNodesBySourceId",
    summary="source_idë¡œ ë…¸ë“œ ì¡°íšŒ",
    description="íŠ¹ì • source_idê°€ descriptionsì— í¬í•¨ëœ ëª¨ë“  ë…¸ë“œì˜ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
    response_description="ë…¸ë“œ ì´ë¦„ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
async def get_nodes_by_source_id(source_id: str, brain_id: str):
    """
    source_idë¡œ ë…¸ë“œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤:
    
    - **source_id**: ì°¾ì„ source_id
    - **brain_id**: ë¸Œë ˆì¸ ID
    
    ë°˜í™˜ê°’:
    - **nodes**: ë…¸ë“œ ì´ë¦„ ëª©ë¡
    """
    logging.info(f"getNodesBySourceId ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ - source_id: {source_id}, brain_id: {brain_id}")
    try:
        # ê·¸ë˜í”„ DBì—ì„œ source_idê°€ í¬í•¨ëœ ë…¸ë“œ ì¡°íšŒ
        neo4j_handler = Neo4jHandler()
        logging.info("Neo4j í•¸ë“¤ëŸ¬ ìƒì„±ë¨")
        
        # Neo4jì—ì„œ source_idë¡œ ë…¸ë“œ ì¡°íšŒ
        node_names = neo4j_handler.get_nodes_by_source_id(source_id, brain_id)
        logging.info(f"ì¡°íšŒëœ ë…¸ë“œ ì´ë¦„: {node_names}")
        
        return {"nodes": node_names}
        
    except Exception as e:
        logging.error("ë…¸ë“œ ì¡°íšŒ ì˜¤ë¥˜: %s", str(e))
        raise HTTPException(status_code=500, detail=f"ë…¸ë“œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.get("/getSourceDataMetrics/{brain_id}",
    summary="ë¸Œë ˆì¸ì˜ ì†ŒìŠ¤ë³„ ë°ì´í„° ë©”íŠ¸ë¦­ ì¡°íšŒ",
    description="íŠ¹ì • ë¸Œë ˆì¸ì˜ ëª¨ë“  ì†ŒìŠ¤ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì–‘ê³¼ ê·¸ë˜í”„ ë°ì´í„° ì–‘ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.",
    response_description="ì†ŒìŠ¤ë³„ í…ìŠ¤íŠ¸ ì–‘ê³¼ ê·¸ë˜í”„ ë°ì´í„° ì–‘ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
    responses={
        404: ErrorExamples[40401],
        500: ErrorExamples[50001]
    }
)
async def get_source_data_metrics(brain_id: str):
    """
    íŠ¹ì • ë¸Œë ˆì¸ì˜ ëª¨ë“  ì†ŒìŠ¤ì— ëŒ€í•œ ë°ì´í„° ë©”íŠ¸ë¦­ì„ ë°˜í™˜í•©ë‹ˆë‹¤:
    
    - **brain_id**: ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•  ë¸Œë ˆì¸ ID
    
    ë°˜í™˜ê°’:
    - **total_text_length**: ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´
    - **total_nodes**: ì „ì²´ ë…¸ë“œ ìˆ˜
    - **total_edges**: ì „ì²´ ì—£ì§€ ìˆ˜
    - **source_metrics**: ì†ŒìŠ¤ë³„ ìƒì„¸ ë©”íŠ¸ë¦­
    """
    logging.info(f"getSourceDataMetrics ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ - brain_id: {brain_id}")
    try:
        neo4j_handler = Neo4jHandler()
        db_handler = SQLiteHandler()
        
        # 1. Neo4jì—ì„œ ê·¸ë˜í”„ ë°ì´í„° ì¡°íšŒ
        graph_data = neo4j_handler.get_brain_graph(brain_id)
        total_nodes = len(graph_data.get('nodes', []))
        total_edges = len(graph_data.get('links', []))
        
        # 2. SQLiteì—ì„œ ì†ŒìŠ¤ë³„ í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
        source_metrics = []
        total_text_length = 0
        
        # PDF ì†ŒìŠ¤ë“¤ ì¡°íšŒ
        pdfs = db_handler.get_pdfs_by_brain(brain_id)
        for pdf in pdfs:
            try:
                # PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨í•œ ì¶”ì •)
                # ì‹¤ì œë¡œëŠ” PDF íŒŒì‹±ì´ í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” íŒŒì¼ í¬ê¸°ë¡œ ì¶”ì •
                import os
                if os.path.exists(pdf['pdf_path']):
                    file_size = os.path.getsize(pdf['pdf_path'])
                    # PDF íŒŒì¼ í¬ê¸°ë¥¼ í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ì¶”ì • (ëŒ€ëµì ì¸ ê³„ì‚°)
                    estimated_text_length = int(file_size * 0.1)  # PDFì˜ ì•½ 10%ê°€ í…ìŠ¤íŠ¸ë¼ê³  ê°€ì •
                else:
                    estimated_text_length = 0
                
                # ì´ PDFì—ì„œ ìƒì„±ëœ ë…¸ë“œ ìˆ˜ ê³„ì‚°
                pdf_nodes = neo4j_handler.get_nodes_by_source_id(pdf['pdf_id'], brain_id)
                pdf_edges = neo4j_handler.get_edges_by_source_id(pdf['pdf_id'], brain_id)
                
                source_metrics.append({
                    "source_id": pdf['pdf_id'],
                    "source_type": "pdf",
                    "title": pdf['pdf_title'],
                    "text_length": estimated_text_length,
                    "nodes_count": len(pdf_nodes),
                    "edges_count": len(pdf_edges)
                })
                
                total_text_length += estimated_text_length
                
            except Exception as e:
                logging.error(f"PDF ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜ (ID: {pdf['pdf_id']}): {str(e)}")
        
        # TXT ì†ŒìŠ¤ë“¤ ì¡°íšŒ
        txts = db_handler.get_textfiles_by_brain(brain_id)
        for txt in txts:
            try:
                # TXT íŒŒì¼ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
                import os
                if os.path.exists(txt['txt_path']):
                    with open(txt['txt_path'], 'r', encoding='utf-8') as f:
                        text_content = f.read()
                        text_length = len(text_content)
                else:
                    text_length = 0
                
                # ì´ TXTì—ì„œ ìƒì„±ëœ ë…¸ë“œ ìˆ˜ ê³„ì‚°
                txt_nodes = neo4j_handler.get_nodes_by_source_id(txt['txt_id'], brain_id)
                txt_edges = neo4j_handler.get_edges_by_source_id(txt['txt_id'], brain_id)
                
                source_metrics.append({
                    "source_id": txt['txt_id'],
                    "source_type": "txt",
                    "title": txt['txt_title'],
                    "text_length": text_length,
                    "nodes_count": len(txt_nodes),
                    "edges_count": len(txt_edges)
                })
                
                total_text_length += text_length
                
            except Exception as e:
                logging.error(f"TXT ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜ (ID: {txt['txt_id']}): {str(e)}")
        
        # MEMO ì†ŒìŠ¤ë“¤ ì¡°íšŒ
        memos = db_handler.get_memos_by_brain(brain_id, is_source=True)
        for memo in memos:
            try:
                # ë©”ëª¨ í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
                text_length = len(memo['memo_text'] or '')
                
                # ì´ ë©”ëª¨ì—ì„œ ìƒì„±ëœ ë…¸ë“œ ìˆ˜ ê³„ì‚°
                memo_nodes = neo4j_handler.get_nodes_by_source_id(memo['memo_id'], brain_id)
                memo_edges = neo4j_handler.get_edges_by_source_id(memo['memo_id'], brain_id)
                
                source_metrics.append({
                    "source_id": memo['memo_id'],
                    "source_type": "memo",
                    "title": memo['memo_title'],
                    "text_length": text_length,
                    "nodes_count": len(memo_nodes),
                    "edges_count": len(memo_edges)
                })
                
                total_text_length += text_length
                
            except Exception as e:
                logging.error(f"MEMO ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜ (ID: {memo['memo_id']}): {str(e)}")
        
        return {
            "total_text_length": total_text_length,
            "total_nodes": total_nodes,
            "total_edges": total_edges,
            "source_metrics": source_metrics
        }
        
    except AppException as ae:
        raise ae
    except Exception as e:
        logging.error("ì†ŒìŠ¤ ë°ì´í„° ë©”íŠ¸ë¦­ ì¡°íšŒ ì˜¤ë¥˜: %s", str(e))
        raise Neo4jException(message=str(e))

@router.get("/sourceCount/{brain_id}", summary="ë¸Œë ˆì¸ë³„ ì „ì²´ ì†ŒìŠ¤ ê°œìˆ˜ ì¡°íšŒ", description="íŠ¹ì • ë¸Œë ˆì¸ì— ì†í•œ PDF, TXT, MD, MEMO ì†ŒìŠ¤ì˜ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
async def get_source_count(brain_id: int):
    """
    í•´ë‹¹ brain_idì— ì†í•œ ëª¨ë“  ì†ŒìŠ¤(PDF, TXT, MD, MEMO) ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    is_sourceê°€ trueì¸ ë©”ëª¨ë§Œ ì†ŒìŠ¤ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    db = SQLiteHandler()
    try:
        pdfs = db.get_pdfs_by_brain(brain_id)
        txts = db.get_textfiles_by_brain(brain_id)
        mds = db.get_mds_by_brain(brain_id)
        memos = db.get_memos_by_brain(brain_id, is_source=True)  # is_sourceê°€ Trueì¸ ë©”ëª¨ë§Œ ì¡°íšŒ
        total_count = len(pdfs) + len(txts) + len(mds) + len(memos)
        return {
            "pdf_count": len(pdfs),
            "txt_count": len(txts),
            "md_count": len(mds),
            "memo_count": len(memos),
            "total_count": total_count
        }
    except Exception as e:
        raise HTTPException(500, f"ì†ŒìŠ¤ ê°œìˆ˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@router.get("/getSourceContent",
    summary="ì†ŒìŠ¤ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ë‚´ìš© ì¡°íšŒ",
    description="íŠ¹ì • source_idì˜ íŒŒì¼ íƒ€ì…ì— ë”°ë¼ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
    response_description="ì†ŒìŠ¤ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
    responses={
        404: ErrorExamples[40401],
        500: ErrorExamples[50001]
    }
)
async def get_source_content(source_id: str, brain_id: str):
    """
    ì†ŒìŠ¤ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì¡°íšŒí•©ë‹ˆë‹¤:
    
    - **source_id**: ì¡°íšŒí•  ì†ŒìŠ¤ ID
    - **brain_id**: ë¸Œë ˆì¸ ID
    
    ë°˜í™˜ê°’:
    - **content**: ì†ŒìŠ¤ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©
    - **title**: ì†ŒìŠ¤ íŒŒì¼ì˜ ì œëª©
    - **type**: íŒŒì¼ íƒ€ì… (pdf, textfile, memo, md, docx)
    """
    logging.info(f"getSourceContent ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ - source_id: {source_id}, brain_id: {brain_id}")
    try:
        db = SQLiteHandler()
        
        # PDFì™€ TextFile, Memo, MD, DocsFile í…Œì´ë¸”ì—ì„œ ëª¨ë‘ ì¡°íšŒ
        pdf = db.get_pdf(int(source_id))
        textfile = db.get_textfile(int(source_id))
        memo = db.get_memo(int(source_id))
        md = db.get_mdfile(int(source_id))
        docx = db.get_docxfile(int(source_id))
        
        content = None
        title = None
        file_type = None
        
        if pdf:
            content = pdf.get('pdf_text', '')
            title = pdf.get('pdf_title', '')
            file_type = 'pdf'
        elif textfile:
            content = textfile.get('txt_text', '')
            title = textfile.get('txt_title', '')
            file_type = 'textfile'
        elif memo:
            content = memo.get('memo_text', '')
            title = memo.get('memo_title', '')
            file_type = 'memo'
        elif md:
            content = md.get('md_text', '')
            title = md.get('md_title', '')
            file_type = 'md'
        elif docx:
            content = docx.get('docx_text', '')
            title = docx.get('docx_title', '')
            file_type = 'docx'
        
        if content is None:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ source_idì˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return {
            "content": content,
            "title": title,
            "type": file_type
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logging.error("ì†ŒìŠ¤ ë‚´ìš© ì¡°íšŒ ì˜¤ë¥˜: %s", str(e))
        raise HTTPException(status_code=500, detail=f"ì†ŒìŠ¤ ë‚´ìš© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")