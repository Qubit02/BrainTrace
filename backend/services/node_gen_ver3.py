"""
í‰ê·  ë²¡í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í† í”½ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.
í‰ê·  ë²¡í„°ì™€ ê°€ê¹Œìš´ ìƒìœ„ 10ê°œì˜ í‚¤ì›Œë“œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
ê° ëª…ì‚¬êµ¬ë¥¼ ì„ë² ë”©í•  ë•Œ ì²« ë“±ì¥í•œ ë¬¸ì¥ì˜ contextë§Œì„ ë°˜ì˜í•˜ëŠ” ë¬¸ì œì ì„ ìˆ˜ì •í•©ë‹ˆë‹¤
ê° ëª…ì‚¬êµ¬ê°€ ë“±ì¥í•œ ëª¨ë“  ë¬¸ì¥ì˜ ì„ë² ë”© ë²¡í„°ì˜ í‰ê· ê°’ì´ í•´ë‹¹ ëª…ì‚¬êµ¬ì˜ context vectorê°€ ë©ë‹ˆë‹¤
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging

#pip install konlpy, pip install transformers torch scikit-learn

import re
import torch
from collections import defaultdict
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from collections import Counter

MODEL_NAME = "nlpai-lab/KoE5"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)
model.eval()

stopwords = set([
    "ì‚¬ì‹¤", "ê²½ìš°", "ì‹œì ˆ", "ë‚´ìš©", "ì ", "ê²ƒ", "ìˆ˜", "ë•Œ", "ì •ë„", "ì´ìœ ", "ìƒí™©", "ë¿", "ë§¤ìš°", "ì•„ì£¼", "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜"
])

okt = Okt()

def is_pronoun_like_phrase(tokens: list[str]) -> bool:
    pronouns = {
        "ì´", "ê·¸", "ì €", "ë³¸", "í•´ë‹¹", "ê·¸ëŸ°", "ì „ìˆ í•œ", "ì•ì„œ", "ë§í•œ", "ì´ëŸ°", "ì´ëŸ¬í•œ", "ê·¸ëŸ¬í•œ"
    }
    
    if len(tokens) <2:
        return False
    
    check_len = min(2, len(tokens))
    for i in range(check_len):
        if tokens[i] in pronouns:
            return True
    return False


def sliding_windows(sentences: list[str], window_size: int =2, stride: int = 1) -> list[str]:
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¬¸ë§¥ ì°½ì„ ë§Œë“­ë‹ˆë‹¤.
    
    Args:
        sentences (list[str]): ë¶„ë¦¬ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        window_size (int): í•œ ìœˆë„ìš°ì— í¬í•¨ë  ë¬¸ì¥ ìˆ˜
        stride (int): ë‹¤ìŒ ìœˆë„ìš°ë¡œ ì´ë™í•  ë¬¸ì¥ ìˆ˜

    Returns:
        list[str]: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í˜•íƒœë¡œ ë¬¶ì¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    windows = []
    for i in range(0, len(sentences) - window_size + 1, stride):
        window = sentences[i:i + window_size+1]
        windows.append(window)
    return windows

def extract_noun_phrases(text: str) -> list:
    """
    ëª…ì‚¬êµ¬ë¥¼ ì¶”ì¶œí•˜ê³ , ì§€ì‹œì–´(ëŒ€ëª…ì‚¬ì„±) ëª…ì‚¬êµ¬ ì—¬ë¶€ë¥¼ íŒë³„í•´ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        List of dicts like:
        [
            {"phrase": "RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ", "is_pronoun_like": False},
            {"phrase": "ì´ ì‹œìŠ¤í…œ", "is_pronoun_like": True}
        ]
    """
    words = okt.pos(text, norm=True, stem=True)
    phrases=[]
    current_phrase=[]

    for word, tag in words:
        if '\n' in word:
            continue
        elif tag in ["Noun", "Alpha"]:
            if word not in stopwords and len(word) > 1:
                current_phrase.append(word)
        elif tag == "Adjective" and word[-1] not in 'ë‹¤ìš”ì£ ':
            current_phrase.append(word)
        else:
            if current_phrase:
                phrase = " ".join(current_phrase)
                phrases.append(phrase)
                current_phrase = []

    if current_phrase:
        phrase = " ".join(current_phrase)
        phrases.append(phrase)

    return phrases


def get_embedding(text: str) -> np.ndarray:
    """
    KoE5 ëª¨ë¸ë¡œ ì…ë ¥ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ([CLS] ë²¡í„° ì‚¬ìš©)
    """
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í°ì˜ ë²¡í„°
    return cls_embedding.squeeze().cpu().numpy()

def compute_scores(
    phrase_info: List[dict], 
    windows: List[List[str]]
) -> tuple[Dict[str, tuple[float, np.ndarray]], List[str], np.ndarray]:

    scores = {}
    phrase_to_indices = defaultdict(set)

    # ê° phraseê°€ ë“±ì¥í•œ window ì¸ë±ìŠ¤ë¥¼ ìˆ˜ì§‘
    for info in phrase_info:
        phrase_to_indices[info["phrase"]].add(info["window_index"])

    total_windows = len(windows)

    phrase_embeddings = {}
    central_vecs = []

    # phraseë³„ í‰ê·  ì„ë² ë”© ê³„ì‚°
    for phrase, indices in phrase_to_indices.items():
        emb_list = []

        for idx in indices:
            context = " ".join(windows[idx])
            highlighted = context.replace(phrase, f"[{phrase}]")

            # get_embeddingì´ ëŠë¦¬ë¯€ë¡œ ìºì‹±í•˜ê±°ë‚˜ ë³‘ë ¬ ì²˜ë¦¬ ê³ ë ¤ ê°€ëŠ¥
            emb = get_embedding(highlighted)
            emb_list.append(emb)

        avg_emb = np.mean(emb_list, axis=0)
        tf = len(indices) / total_windows

        phrase_embeddings[phrase] = (tf, avg_emb)
        central_vecs.append(avg_emb)

    # ì¤‘ì‹¬ ë²¡í„° ê³„ì‚°
    central_vec = np.mean(central_vecs, axis=0)

    # vector stack for cosine_similarity
    phrases = list(phrase_embeddings.keys())
    tf_list = []
    emb_list = []

    for phrase in phrases:
        tf, emb = phrase_embeddings[phrase]
        tf_adj = tf * cosine_similarity([emb], [central_vec])[0][0]
        scores[phrase] = [tf_adj, emb]
        tf_list.append(tf_adj)
        emb_list.append(emb)

    topic = sorted(scores.items(), key=lambda x: x[1][0], reverse=True)[:10]

    emb_array = np.stack(emb_list)
    sim_matrix = cosine_similarity(emb_array)

    return scores, phrases, sim_matrix, topic


def group_phrases(
    phrases: List[str],
    phrase_scores: List[dict],
    sim_matrix: np.ndarray,
    threshold: float = 0.98
) -> list[Dict]:
    ungrouped = list(range(len(phrases)))  # ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ
    groups = []

    while ungrouped:
        i = ungrouped.pop()
        group = [i]
        to_check = set()

        for j in ungrouped:
            if sim_matrix[i][j] >= threshold:
                to_check.add(j)

        valid_members = []
        for j in to_check:
            if all(sim_matrix[j][k] >= threshold for k in group):
                valid_members.append(j)

        for j in valid_members:
            group.append(j)
            ungrouped.remove(j)

        groups.append(group)

    # ëŒ€í‘œ ëª…ì‚¬êµ¬ ì„¤ì •: ì¤‘ì‹¬ì„± ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ê²ƒ
    group_infos = []
    for group in groups:
        best_idx = max(group, key=lambda idx: phrase_scores[phrases[idx]][0])
        group_infos.append({
            "representative": phrases[best_idx],
            "members": [phrases[idx] for idx in group]
        })

    return group_infos


def normalize_coreferences(text: str) -> str:
    phrase_info=[]
    sentences = re.split(r'(?<=[.!?])\s+|(?<=[ë‹¤ìš”ì£ ì˜¤])\s*$|\n', text.strip(), flags=re.MULTILINE)
    sentences=[s.strip() for s in sentences if s.strip()]
    windows = sliding_windows(sentences, window_size=2, stride=1)


    # ê° ë¬¸ì¥ì—ì„œ ëª…ì‚¬êµ¬ ì¶”ì¶œ
    for w_idx, window in enumerate(windows):
        
        if w_idx==0:
            phrases=extract_noun_phrases(window[0])
            s_idx=0
        else:
            phrases=extract_noun_phrases(window[1])
            s_idx=1
        s_idx += w_idx

        for p in phrases:
            phrase_info.append({
                "window_index": w_idx,
                "sentence_index": s_idx,
                "phrase": p
            })
 
    phrase_scores, phrases, sim_matrix, topic=compute_scores(phrase_info, windows)
    groups=group_phrases(phrases, phrase_scores, sim_matrix)

    print(topic)
    print("--------------------------------------------------")
    print("ğŸ“Œ ëª…ì‚¬êµ¬ ê·¸ë£¹ ì •ë³´:")
    for g in groups:
        print(f"{g['representative']} â† {g['members']}")
    

texts="""â‘  ì•¼ì„±ì , í™œë™ì , ì •ì—´ì 
ê³ ë ¤ëŒ€í•™êµì˜ êµí’ì€ ì•¼ì„±, í™œê¸°ì™€ ì •ì—´ ë“±ìœ¼ë¡œ ëŒ€í‘œëœë‹¤. ë¬´ì„­ê³  ì‚¬ë‚˜ìš´ í˜¸ë‘ì´, ê°•ë ¬í•˜ê²Œ ê²€ë¶‰ì€ í¬ë¦¼ìŠ¨ìƒ‰ ë“± ê³ ëŒ€ë¥¼ ëŒ€í‘œí•˜ê±°ë‚˜ 'ê³ ëŒ€' í•˜ë©´ ë– ì˜¤ë¥´ëŠ” ìƒì§•ë“¤ì€ ëŒ€ë¶€ë¶„ ìœ„ì˜ íŠ¹ì§•ë“¤ê³¼ ì—°ê´€ëœ ê²½ìš°ê°€ ë§ë‹¤. ì´ëŠ” ê³ ë ¤ëŒ€í•™êµê°€ ê·¸ ì „ì‹ ì¸ ë³´ì„±ì „ë¬¸í•™êµ ì‹œì ˆ ì‚¬ì‹¤ìƒ ìœ ì¼í•œ ë¯¼ì¡±Â·ë¯¼ë¦½ì˜ ì§€ë„ì ì–‘ì„±ê¸°êµ¬ì˜€ê¸° ë•Œë¬¸ì—, ë¯¼ì¡±ì •ì‹ ì´ë¼ëŠ” ì‹œëŒ€ì  ìš”êµ¬ê°€ êµìˆ˜ì™€ í•™ìƒë“¤ì—ê²Œ íŠ¹ë³„íˆ ë” ë¶€í•˜ëê³ , ê·¸ê²ƒì´ í•™ìƒë“¤ì˜ ì§€ì‚¬ì  ë˜ëŠ” íˆ¬ì‚¬ì  ì €í•­ ê¸°ì§ˆì„ ë°°íƒœì‹œì¼°ë˜ ë° ê¸°ì¸í•œë‹¤ëŠ” ê²¬í•´ê°€ ìˆë‹¤.[20]

â‘¡ í˜‘ë™ì , ëˆëˆí•¨
ê³ ë ¤ëŒ€ì—ì„œëŠ” ì¡¸ì—…ìƒì„ 'ë™ë¬¸', 'ë™ì°½' ë“±ì˜ ë‹¨ì–´ ëŒ€ì‹  'êµìš°(æ ¡å‹)'ë¼ê³  ë¶€ë¥¸ë‹¤. ì´ëŠ” í•™êµë¥¼ ê°™ì´ ë‹¤ë…”ë‹¤ëŠ” ì´ìœ ë§Œìœ¼ë¡œ ì¹œêµ¬ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. ì‚¬íšŒì—ì„œ ê³ ë ¤ëŒ€ ì¶œì‹  ê°„ì—ëŠ” ìœ ëŒ€ê°€ ë§¤ìš° ê°•í•œ í¸ì´ë©° ì´ëŸ¬í•œ ë¬¸í™”ëŠ” ê°œì¸ì£¼ì˜ ì„±í–¥ì´ ê°•í•´ì§„ í˜„ëŒ€ì—ë„ ì‚¬ë¼ì§€ì§€ ì•Šê³  ê±´ê°•í•˜ê²Œ ì´ì–´ì§€ê³  ìˆë‹¤. ê³ ëŒ€ì—ëŠ” ìê¸° ì´ìµë§Œ ì•ì„¸ìš°ë ¤ í•˜ê¸°ë³´ë‹¤ëŠ”, íƒ€ì¸ê³¼ ì†Œí†µí•˜ê³  ì„œë¡œì˜ ì¥ì ì„ ì‚´ë ¤ ì¼ì„ ë¶„ë‹´í•¨ìœ¼ë¡œì¨ ì‹œë„ˆì§€ë¥¼ ë‚´ëŠ” ë¬¸í™”ê°€ ë°œë‹¬ë¼ ìˆë‹¤. ë˜í•œ ì¼ëŒ€ì¼ ê°„ì˜ ê´€ê³„ë³´ë‹¤ëŠ” í­ë„“ì€ ì§‘ë‹¨ ë‚´ì—ì„œì˜ ê´€ê³„ë¥¼ ë” ì„ í˜¸í•˜ëŠ” í¸ì´ë‹¤.[21] êµ¬ì„±ì›ë“¤ì˜ ì• êµì‹¬ì´ ì›Œë‚™ ì»¤ì„œ ê·¸ëŸ°ì§€, ì •ì¹˜ì  ì´ë… ë° ê²½ì œì  ì´í•´ê´€ê³„ê°€ ë‹¤ë¥´ë”ë¼ë„ ê°™ì€ ê³ ëŒ€ ë™ë¬¸ ì‚¬ì´ì—ëŠ” ì¢€ ë” ìƒëŒ€ë°©ì˜ ì…ì¥ì— ì„œì„œ ìƒê°í•´ë³´ê³  ì¸ê°„ì  ì‹ ë¢°ì— ì…ê°í•˜ì—¬ ê°ˆë“±ì„ í’€ì–´ê°€ë ¤ëŠ” ì „í†µì´ ì´ì–´ì§€ê³  ìˆë‹¤. ì‹¤ì œë¡œ ê³ ë ¤ëŒ€ì—ëŠ” ë™ì•„ë¦¬ ì¡°ì§ì´ ë°œë‹¬í•´ ê·¸ êµ¬ì„±ì›ì´ ì¸ê°„ê´€ê³„ë¥¼ ë‹¤ì§€ê³  íŒ€í”Œë ˆì´ë¥¼ í•˜ëŠ” í’ì¡°ê°€ ê°•í•˜ë‹¤. ê³µë¶€ë„ ë¬¼ë¡  ì¤‘ìš”ì‹œí•˜ì§€ë§Œ, ê°œì¸ì˜ ì„±ì ë§Œì„ ì±™ê¸°ëŠ” ëŠ¥ë ¥ë³´ë‹¤ëŠ” ì¸ê°„ê´€ê³„ë¥¼ ì¶©ì‹¤íˆ í•˜ëŠ” ëŠ¥ë ¥, ë‚¨ì„ ì´ë„ëŠ” ì§€ë„ë ¥ì´ë‚˜ ìƒê¸‰ì, ë™ë£Œì™€ í™”í•©í•˜ëŠ” ì¹œí™”ë ¥ ë“±ì„ ë” ë†’ì´ í‰ê°€í•˜ëŠ” í¸ì´ë‹¤. ë‹¤ë¥¸ ê·¸ ë¬´ì—‡ë³´ë‹¤ë„ ì¥ê¸°ì ì¸ ëŒ€ì¸ê´€ê³„ì™€ ì‹ ë¢°ê°ì„ ì¤‘ì‹œí•˜ëŠ” ìŠµê´€, ì¡°ì§ì„ ìœ„í•´ í¬ìƒí•˜ê³  ë´‰ì‚¬í•˜ê³  ì˜¤ìš• ë’¤ì§‘ì–´ì“°ëŠ” ì¼ì„ ë‘ë ¤ì›Œí•˜ì§€ ì•ŠëŠ” ê¸°ì§ˆì´ ì´ëŸ° ë¬¸í™” ì†ì—ì„œ ê¸¸ëŸ¬ì§€ëŠ” ê±´ ë‹¹ì—°í•œ ì¼ì´ë‹¤.[22] 21ì„¸ê¸° ë“¤ì–´ì„œ ì˜¤í”„ë¼ì¸ ì»¤ë„¥ì…˜ë§Œì´ ì•„ë‹ˆë¼ ì˜¨ë¼ì¸ ì»¤ë„¥ì…˜ì˜ ì¤‘ìš”ì„±ì´ ë§¤ìš° ì»¤ì¡ŒëŠ”ë°, ì´ì— ë°œë§ì¶° ê³ ëŒ€ì—ì„œëŠ” ì¸í„°ë„· ì»¤ë®¤ë‹ˆí‹°ë„ ë§¤ìš° í™œë°œí•˜ê²Œ ìš´ì˜ë˜ê³  ìˆë‹¤. ê³ ë ¤ëŒ€í•™êµ ì—ë¸Œë¦¬íƒ€ì„ë„ ìƒë‹¹íˆ í™œì„±í™”ë˜ì–´ ìˆëŠ” í¸ì´ì§€ë§Œ, íŠ¹íˆ ê³ ëŒ€ì˜ ìë‘ ì¤‘ í•˜ë‚˜ì¸ ê³ íŒŒìŠ¤ì˜ ê²½ìš° ê°ì¢… ê²Œì‹œíŒì—ì„œ ìœ í†µë˜ê³  ëˆ„ì ë˜ëŠ” ì •ë³´ê°€ ë§¤ìš° ë°©ëŒ€í•  ë¿ ì•„ë‹ˆë¼ ì˜ì–‘ê°€ë„ ë†’ë‹¤.[23]"""
results=normalize_coreferences(texts)

