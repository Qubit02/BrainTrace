"""
ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ gpu ì—°ì‚°ì„ ë„ì…í•˜ê³ , ì„ë² ë”©ì„ ë³‘ë ¬ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging

#pip install konlpy, pip install transformers torch scikit-learn

import re
import torch
from collections import defaultdict
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
from tqdm import tqdm
from collections import Counter

MODEL_NAME = "nlpai-lab/KoE5"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)
model.eval()

stopwords = set([
    "ì‚¬ì‹¤", "ê²½ìš°", "ì‹œì ˆ", "ë‚´ìš©", "ì ", "ê²ƒ", "ìˆ˜", "ë•Œ", "ì •ë„", "ì´ìœ ", "ìƒí™©", "ë¿", "ë§¤ìš°", "ì•„ì£¼", "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜"
])

okt = Okt()

def is_pronoun_like_phrase(tokens: list[str]) -> bool:
    pronouns = {
        "ì´", "ê·¸", "ì €", "ë³¸", "í•´ë‹¹", "ê·¸ëŸ°", "ì „ìˆ í•œ", "ì•ì„œ", "ë§í•œ", "ì´ëŸ°", "ì´ëŸ¬í•œ", "ê·¸ëŸ¬í•œ"
    }
    
    if len(tokens) <2:
        return False
    
    check_len = min(2, len(tokens))
    for i in range(check_len):
        if tokens[i] in pronouns:
            return True
    return False

def extract_noun_phrases(text: str) -> list:
    """
    ëª…ì‚¬êµ¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    """
    words = okt.pos(text, norm=True, stem=True)
    phrases=[]
    current_phrase=[]

    for word, tag in words:
        if '\n' in word:
            continue
        elif tag in ["Noun", "Alpha"]:
            if word not in stopwords and len(word) > 1:
                current_phrase.append(word)
        elif tag == "Adjective" and word[-1] not in 'ë‹¤ìš”ì£ ':
            current_phrase.append(word)
        else:
            if current_phrase:
                phrase = " ".join(current_phrase)
                phrases.append(phrase)
                current_phrase = []

    if current_phrase:
        phrase = " ".join(current_phrase)
        phrases.append(phrase)

    return phrases

# í•œ phraseì˜ ì„ë² ë”©ì„ ê³„ì‚°í•˜ëŠ” ë³‘ë ¬ ì²˜ë¦¬ìš© í•¨ìˆ˜
def compute_phrase_embedding(phrase: str, indices: List[int], sentences: List[str], total_sentences: int) -> tuple:
    highlighted_texts = [sentences[idx].replace(phrase, f"[{phrase}]") for idx in indices]
    embeddings = get_embeddings_batch(highlighted_texts)
    avg_emb = np.mean(embeddings, axis=0)
    tf = len(indices) / total_sentences
    return phrase, (tf, avg_emb)

def get_embeddings_batch(texts: List[str]) -> np.ndarray:
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í° ì„ë² ë”©
    return cls_embeddings.cpu().numpy()

def compute_scores(
    phrase_info: List[dict], 
    sentences: List[str]
) -> tuple[Dict[str, tuple[float, np.ndarray]], List[str], np.ndarray]:

    scores = {}
    phrase_to_indices = defaultdict(set)

    # ê° phraseê°€ ë“±ì¥í•œ sentence ì¸ë±ìŠ¤ë¥¼ ìˆ˜ì§‘
    for info in phrase_info:
        phrase_to_indices[info["phrase"]].add(info["sentence_index"])

    total_sentences = len(sentences)

    phrase_embeddings = {}
    central_vecs = []

        # phraseë³„ í‰ê·  ì„ë² ë”© ê³„ì‚°
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [
            executor.submit(compute_phrase_embedding, phrase, indices, sentences, total_sentences)
            for phrase, indices in phrase_to_indices.items()
        ]

        for future in tqdm(as_completed(futures), total=len(futures), desc="Embedding phrases"):
            phrase, (tf, avg_emb) = future.result()
            phrase_embeddings[phrase] = (tf, avg_emb)
            central_vecs.append(avg_emb)

    # ì¤‘ì‹¬ ë²¡í„° ê³„ì‚°
    central_vec = np.mean(central_vecs, axis=0)

    # vector stack for cosine_similarity
    phrases = list(phrase_embeddings.keys())
    tf_list = []
    emb_list = []

    for phrase in phrases:
        tf, emb = phrase_embeddings[phrase]
        tf_adj = tf * cosine_similarity([emb], [central_vec])[0][0]
        scores[phrase] = [tf_adj, emb]
        tf_list.append(tf_adj)
        emb_list.append(emb)

    topic = sorted(scores.items(), key=lambda x: x[1][0], reverse=True)[:10]

    emb_array = np.stack(emb_list)
    sim_matrix = cosine_similarity(emb_array)

    return scores, phrases, sim_matrix, topic


def group_phrases(
    phrases: List[str],
    phrase_scores: List[dict],
    sim_matrix: np.ndarray,
    threshold: float = 0.98
) -> list[Dict]:
    ungrouped = list(range(len(phrases)))  # ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ
    groups = []

    while ungrouped:
        i = ungrouped.pop()
        group = [i]
        to_check = set()

        for j in ungrouped:
            if sim_matrix[i][j] >= threshold:
                to_check.add(j)

        valid_members = []
        for j in to_check:
            if all(sim_matrix[j][k] >= threshold for k in group):
                valid_members.append(j)

        for j in valid_members:
            group.append(j)
            ungrouped.remove(j)

        groups.append(group)

    # ëŒ€í‘œ ëª…ì‚¬êµ¬ ì„¤ì •: ì¤‘ì‹¬ì„± ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ê²ƒ
    group_infos = []
    for group in groups:
        best_idx = max(group, key=lambda idx: phrase_scores[phrases[idx]][0])
        group_infos.append({
            "representative": phrases[best_idx],
            "members": [phrases[idx] for idx in group]
        })

    return group_infos


def normalize_coreferences(text: str) -> str:
    phrase_info=[]
    sentences = re.split(r'(?<=[.!?])\s+|(?<=[ë‹¤ìš”ì£ ì˜¤])\s*$|\n', text.strip(), flags=re.MULTILINE)
    sentences=[s.strip() for s in sentences if s.strip()]

    # ê° ë¬¸ì¥ì—ì„œ ëª…ì‚¬êµ¬ ì¶”ì¶œ
    for s_idx, sentence in enumerate(sentences):
        phrases=extract_noun_phrases(sentence)
        for p in phrases:
            phrase_info.append({
                "sentence_index": s_idx,
                "phrase": p
            })
 
    phrase_scores, phrases, sim_matrix, topic=compute_scores(phrase_info, sentences)
    groups=group_phrases(phrases, phrase_scores, sim_matrix)

    print(topic)
    print("--------------------------------------------------")
    print("ğŸ“Œ ëª…ì‚¬êµ¬ ê·¸ë£¹ ì •ë³´:")
    for g in groups:
        print(f"{g['representative']} â† {g['members']}")
    

texts="""â‘  ì•¼ì„±ì , í™œë™ì , ì •ì—´ì 
ê³ ë ¤ëŒ€í•™êµì˜ êµí’ì€ ì•¼ì„±, í™œê¸°ì™€ ì •ì—´ ë“±ìœ¼ë¡œ ëŒ€í‘œëœë‹¤. ë¬´ì„­ê³  ì‚¬ë‚˜ìš´ í˜¸ë‘ì´, ê°•ë ¬í•˜ê²Œ ê²€ë¶‰ì€ í¬ë¦¼ìŠ¨ìƒ‰ ë“± ê³ ëŒ€ë¥¼ ëŒ€í‘œí•˜ê±°ë‚˜ 'ê³ ëŒ€' í•˜ë©´ ë– ì˜¤ë¥´ëŠ” ìƒì§•ë“¤ì€ ëŒ€ë¶€ë¶„ ìœ„ì˜ íŠ¹ì§•ë“¤ê³¼ ì—°ê´€ëœ ê²½ìš°ê°€ ë§ë‹¤. ì´ëŠ” ê³ ë ¤ëŒ€í•™êµê°€ ê·¸ ì „ì‹ ì¸ ë³´ì„±ì „ë¬¸í•™êµ ì‹œì ˆ ì‚¬ì‹¤ìƒ ìœ ì¼í•œ ë¯¼ì¡±Â·ë¯¼ë¦½ì˜ ì§€ë„ì ì–‘ì„±ê¸°êµ¬ì˜€ê¸° ë•Œë¬¸ì—, ë¯¼ì¡±ì •ì‹ ì´ë¼ëŠ” ì‹œëŒ€ì  ìš”êµ¬ê°€ êµìˆ˜ì™€ í•™ìƒë“¤ì—ê²Œ íŠ¹ë³„íˆ ë” ë¶€í•˜ëê³ , ê·¸ê²ƒì´ í•™ìƒë“¤ì˜ ì§€ì‚¬ì  ë˜ëŠ” íˆ¬ì‚¬ì  ì €í•­ ê¸°ì§ˆì„ ë°°íƒœì‹œì¼°ë˜ ë° ê¸°ì¸í•œë‹¤ëŠ” ê²¬í•´ê°€ ìˆë‹¤.[20]

â‘¡ í˜‘ë™ì , ëˆëˆí•¨
ê³ ë ¤ëŒ€ì—ì„œëŠ” ì¡¸ì—…ìƒì„ 'ë™ë¬¸', 'ë™ì°½' ë“±ì˜ ë‹¨ì–´ ëŒ€ì‹  'êµìš°(æ ¡å‹)'ë¼ê³  ë¶€ë¥¸ë‹¤. ì´ëŠ” í•™êµë¥¼ ê°™ì´ ë‹¤ë…”ë‹¤ëŠ” ì´ìœ ë§Œìœ¼ë¡œ ì¹œêµ¬ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. ì‚¬íšŒì—ì„œ ê³ ë ¤ëŒ€ ì¶œì‹  ê°„ì—ëŠ” ìœ ëŒ€ê°€ ë§¤ìš° ê°•í•œ í¸ì´ë©° ì´ëŸ¬í•œ ë¬¸í™”ëŠ” ê°œì¸ì£¼ì˜ ì„±í–¥ì´ ê°•í•´ì§„ í˜„ëŒ€ì—ë„ ì‚¬ë¼ì§€ì§€ ì•Šê³  ê±´ê°•í•˜ê²Œ ì´ì–´ì§€ê³  ìˆë‹¤. ê³ ëŒ€ì—ëŠ” ìê¸° ì´ìµë§Œ ì•ì„¸ìš°ë ¤ í•˜ê¸°ë³´ë‹¤ëŠ”, íƒ€ì¸ê³¼ ì†Œí†µí•˜ê³  ì„œë¡œì˜ ì¥ì ì„ ì‚´ë ¤ ì¼ì„ ë¶„ë‹´í•¨ìœ¼ë¡œì¨ ì‹œë„ˆì§€ë¥¼ ë‚´ëŠ” ë¬¸í™”ê°€ ë°œë‹¬ë¼ ìˆë‹¤. ë˜í•œ ì¼ëŒ€ì¼ ê°„ì˜ ê´€ê³„ë³´ë‹¤ëŠ” í­ë„“ì€ ì§‘ë‹¨ ë‚´ì—ì„œì˜ ê´€ê³„ë¥¼ ë” ì„ í˜¸í•˜ëŠ” í¸ì´ë‹¤.[21] êµ¬ì„±ì›ë“¤ì˜ ì• êµì‹¬ì´ ì›Œë‚™ ì»¤ì„œ ê·¸ëŸ°ì§€, ì •ì¹˜ì  ì´ë… ë° ê²½ì œì  ì´í•´ê´€ê³„ê°€ ë‹¤ë¥´ë”ë¼ë„ ê°™ì€ ê³ ëŒ€ ë™ë¬¸ ì‚¬ì´ì—ëŠ” ì¢€ ë” ìƒëŒ€ë°©ì˜ ì…ì¥ì— ì„œì„œ ìƒê°í•´ë³´ê³  ì¸ê°„ì  ì‹ ë¢°ì— ì…ê°í•˜ì—¬ ê°ˆë“±ì„ í’€ì–´ê°€ë ¤ëŠ” ì „í†µì´ ì´ì–´ì§€ê³  ìˆë‹¤. ì‹¤ì œë¡œ ê³ ë ¤ëŒ€ì—ëŠ” ë™ì•„ë¦¬ ì¡°ì§ì´ ë°œë‹¬í•´ ê·¸ êµ¬ì„±ì›ì´ ì¸ê°„ê´€ê³„ë¥¼ ë‹¤ì§€ê³  íŒ€í”Œë ˆì´ë¥¼ í•˜ëŠ” í’ì¡°ê°€ ê°•í•˜ë‹¤. ê³µë¶€ë„ ë¬¼ë¡  ì¤‘ìš”ì‹œí•˜ì§€ë§Œ, ê°œì¸ì˜ ì„±ì ë§Œì„ ì±™ê¸°ëŠ” ëŠ¥ë ¥ë³´ë‹¤ëŠ” ì¸ê°„ê´€ê³„ë¥¼ ì¶©ì‹¤íˆ í•˜ëŠ” ëŠ¥ë ¥, ë‚¨ì„ ì´ë„ëŠ” ì§€ë„ë ¥ì´ë‚˜ ìƒê¸‰ì, ë™ë£Œì™€ í™”í•©í•˜ëŠ” ì¹œí™”ë ¥ ë“±ì„ ë” ë†’ì´ í‰ê°€í•˜ëŠ” í¸ì´ë‹¤. ë‹¤ë¥¸ ê·¸ ë¬´ì—‡ë³´ë‹¤ë„ ì¥ê¸°ì ì¸ ëŒ€ì¸ê´€ê³„ì™€ ì‹ ë¢°ê°ì„ ì¤‘ì‹œí•˜ëŠ” ìŠµê´€, ì¡°ì§ì„ ìœ„í•´ í¬ìƒí•˜ê³  ë´‰ì‚¬í•˜ê³  ì˜¤ìš• ë’¤ì§‘ì–´ì“°ëŠ” ì¼ì„ ë‘ë ¤ì›Œí•˜ì§€ ì•ŠëŠ” ê¸°ì§ˆì´ ì´ëŸ° ë¬¸í™” ì†ì—ì„œ ê¸¸ëŸ¬ì§€ëŠ” ê±´ ë‹¹ì—°í•œ ì¼ì´ë‹¤.[22] 21ì„¸ê¸° ë“¤ì–´ì„œ ì˜¤í”„ë¼ì¸ ì»¤ë„¥ì…˜ë§Œì´ ì•„ë‹ˆë¼ ì˜¨ë¼ì¸ ì»¤ë„¥ì…˜ì˜ ì¤‘ìš”ì„±ì´ ë§¤ìš° ì»¤ì¡ŒëŠ”ë°, ì´ì— ë°œë§ì¶° ê³ ëŒ€ì—ì„œëŠ” ì¸í„°ë„· ì»¤ë®¤ë‹ˆí‹°ë„ ë§¤ìš° í™œë°œí•˜ê²Œ ìš´ì˜ë˜ê³  ìˆë‹¤. ê³ ë ¤ëŒ€í•™êµ ì—ë¸Œë¦¬íƒ€ì„ë„ ìƒë‹¹íˆ í™œì„±í™”ë˜ì–´ ìˆëŠ” í¸ì´ì§€ë§Œ, íŠ¹íˆ ê³ ëŒ€ì˜ ìë‘ ì¤‘ í•˜ë‚˜ì¸ ê³ íŒŒìŠ¤ì˜ ê²½ìš° ê°ì¢… ê²Œì‹œíŒì—ì„œ ìœ í†µë˜ê³  ëˆ„ì ë˜ëŠ” ì •ë³´ê°€ ë§¤ìš° ë°©ëŒ€í•  ë¿ ì•„ë‹ˆë¼ ì˜ì–‘ê°€ë„ ë†’ë‹¤.[23]"""
results=normalize_coreferences(texts)

