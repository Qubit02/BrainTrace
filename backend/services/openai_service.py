"""
OpenAI ê¸°ë°˜ ê·¸ë˜í”„ ì¶”ì¶œ/ì§ˆì˜ì‘ë‹µ ì„œë¹„ìŠ¤
-------------------------------------

ì´ ëª¨ë“ˆì€ OpenAI APIë¥¼ í™œìš©í•´ í…ìŠ¤íŠ¸ë¡œë¶€í„° ë…¸ë“œ/ì—£ì§€(ê·¸ë˜í”„ êµ¬ì„±ìš”ì†Œ)ë¥¼ ì¶”ì¶œí•˜ê³ ,
ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸(ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸)ì™€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ê¸´ í…ìŠ¤íŠ¸(â‰¥2000ì) ì²­í‚¹ ì²˜ë¦¬ í›„ ê° ì²­í¬ì—ì„œ ë…¸ë“œ/ì—£ì§€ ì¶”ì¶œ
- ì¶”ì¶œëœ ë…¸ë“œì˜ descriptionê³¼ ë¬¸ì¥ ì„ë² ë”© ê°„ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ original_sentences ì‚°ì¶œ
- ìŠ¤í‚¤ë§ˆ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ LLM ì§ˆì˜ì‘ë‹µì— í™œìš©
- ë‹µë³€ì˜ ë§¨ ë(JSON ì˜ì—­)ì—ì„œ referenced_nodesë¥¼ íŒŒì‹±í•˜ì—¬ ë…¸ë“œ ì°¸ì¡° ëª©ë¡ì„ ì¶”ì¶œ

í™˜ê²½ ë³€ìˆ˜:
- OPENAI_API_KEY: OpenAI API í˜¸ì¶œì— ì‚¬ìš© (dotenvë¥¼ í†µí•´ ë¡œë“œ)

ì£¼ì˜:
- ë³¸ ëª¨ë“ˆì€ ì™¸ë¶€ API í˜¸ì¶œì„ í¬í•¨í•˜ë¯€ë¡œ, ì¥ì• /ìš”ê¸ˆ/ë ˆì´íŠ¸ ë¦¬ë°‹ ê³ ë ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.
- ì„ë² ë”©/ìœ ì‚¬ë„ ê³„ì‚°(threshold)ì€ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ, ë„ë©”ì¸ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.
"""


import logging
from openai import OpenAI           # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
import json
from .chunk_service import chunk_text
from .base_ai_service import BaseAIService
from typing import List
from .manual_chunking_sentences import manual_chunking
import numpy as np
import os
from dotenv import load_dotenv  # dotenv ì¶”ê°€
from .embedding_service import encode_text
from sklearn.metrics.pairwise import cosine_similarity



# âœ… .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("âŒ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. generate_answerã….env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ë…¸ë“œ/ì—£ì§€ ì¶”ì¶œì— í™œìš©)
# client = OpenAI(api_key=openai_api_key)
client = OpenAI(api_key=openai_api_key)


class OpenAIService(BaseAIService) :
    """OpenAI APIë¥¼ ì‚¬ìš©í•´ ê·¸ë˜í”„ ì¶”ì¶œ/QAë¥¼ ìˆ˜í–‰í•˜ëŠ” ì„œë¹„ìŠ¤ êµ¬í˜„ì²´."""
    def __init__(self, model_name="gpt-4o"):
        # ì¸ìŠ¤í„´ìŠ¤ ì†ì„±ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ í• ë‹¹
        self.client = OpenAI(api_key=openai_api_key)
        self.model_name = model_name  # ëª¨ë¸ëª… ì €ì¥
    def extract_referenced_nodes(self,llm_response: str) -> List[str]:
        """
        LLM ì‘ë‹µ ë¬¸ìì—´ì—ì„œ EOF ë’¤ì˜ JSONì„ íŒŒì‹±í•˜ì—¬ referenced_nodesë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

        - 'ë ˆì´ë¸”-ë…¸ë“œ' í˜•ì‹ì¼ ê²½ìš° ë ˆì´ë¸”ê³¼ '-'ì„ ì œê±°í•˜ê³  ë…¸ë“œ ì´ë¦„ë§Œ ë°˜í™˜
        - EOF ì´í›„ JSONì´ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        parts = llm_response.split("EOF")
        if len(parts) < 2:
            return []

        json_part = parts[-1].strip()
        try:
            payload = json.loads(json_part)
            # payloadê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            if isinstance(payload, list):
                return []
            # payloadê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°ì—ë§Œ get() í˜¸ì¶œ
            raw_nodes = payload.get("referenced_nodes", [])
            cleaned = [
                node.split("-", 1)[1] if "-" in node else node
                for node in raw_nodes
            ]
            return cleaned
        except json.JSONDecodeError:
            return []

    def extract_graph_components(self,text: str, source_id: str):
        """
        ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ LLMì„ í™œìš©í•´ ë…¸ë“œì™€ ì—£ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        í…ìŠ¤íŠ¸ê°€ 2000ì ì´ìƒì¸ ê²½ìš° ì²­í‚¹í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ë°˜í™˜ í˜•ì‹: (nodes: list, edges: list)
        """
        # ëª¨ë“  ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        all_nodes = []
        all_edges = []
        
        # í…ìŠ¤íŠ¸ê°€ 2000ì ì´ìƒì´ë©´ ì²­í‚¹
        if len(text) >= 2000:
            # ê·œì¹™ ê¸°ë°˜ ìˆ˜ë™ ì²­í‚¹(ë¬¸ì¥ ë‹¨ìœ„)ì„ ì‚¬ìš©
            chunks = manual_chunking(text)
            logging.info(f"âœ… í…ìŠ¤íŠ¸ê°€ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì–´ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
            
            # ê° ì²­í¬ë³„ë¡œ ë…¸ë“œì™€ ì—£ì§€ ì¶”ì¶œ
            for i, chunk in enumerate(chunks, 1):
                logging.info(f"ì²­í¬ {i}/{len(chunks)} ì²˜ë¦¬ ì¤‘...")
                nodes, edges = self._extract_from_chunk(chunk, source_id)
                all_nodes.extend(nodes)
                all_edges.extend(edges)
        else:
            # 2000ì ë¯¸ë§Œì´ë©´ ì§ì ‘ ì²˜ë¦¬
            all_nodes, all_edges = self._extract_from_chunk(text, source_id)
        
        # ì¤‘ë³µ ì œê±°
        all_nodes = self._remove_duplicate_nodes(all_nodes)
        all_edges = self._remove_duplicate_edges(all_edges)
        
        logging.info(f"âœ… ì´ {len(all_nodes)}ê°œì˜ ë…¸ë“œì™€ {len(all_edges)}ê°œì˜ ì—£ì§€ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return all_nodes, all_edges

    def _extract_from_chunk(self, chunk: str, source_id: str):
        """ê°œë³„ ì²­í¬ì—ì„œ ë…¸ë“œ/ì—£ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ë³´ê°•(original_sentences)í•©ë‹ˆë‹¤.

        ì²˜ë¦¬ ë‹¨ê³„:
          1) í”„ë¡¬í”„íŠ¸ êµ¬ì„± â†’ OpenAI í˜¸ì¶œ(ì‘ë‹µì„ JSONìœ¼ë¡œ ê°•ì œ)
          2) ë…¸ë“œ í•„ìˆ˜ í•„ë“œ ê²€ì¦/ì •ê·œí™” + description â†’ descriptionsë¡œ ì´ë™, source_id ì£¼ì…
          3) ì—£ì§€ì˜ source/target ê²€ì¦(ë…¸ë“œ name ì°¸ì¡°)
          4) ë¬¸ì¥ ë‹¨ìœ„ ì²­í‚¹ í›„ ì„ë² ë”© ê³„ì‚°, descriptionê³¼ì˜ ìœ ì‚¬ë„ë¡œ original_sentences êµ¬ì„±
        """
        prompt = (
        "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì„œ ë…¸ë“œì™€ ì—£ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜. "
        "ë…¸ë“œëŠ” { \"label\": string, \"name\": string, \"description\": string } í˜•ì‹ì˜ ê°ì²´ ë°°ì—´, "
        "ì—£ì§€ëŠ” { \"source\": string, \"target\": string, \"relation\": string } í˜•ì‹ì˜ ê°ì²´ ë°°ì—´ë¡œ ì¶œë ¥í•´ì¤˜. "
        "ì—¬ê¸°ì„œ sourceì™€ targetì€ ë…¸ë“œì˜ nameì„ ì°¸ì¡°í•´ì•¼ í•˜ê³ , source_idëŠ” ì‚¬ìš©í•˜ë©´ ì•ˆ ë¼. "
        "ì¶œë ¥ ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ì„ ì¤€ìˆ˜í•´ì•¼ í•´:\n"
        "{\n"
        '  "nodes": [ ... ],\n'
        '  "edges": [ ... ]\n'
        "}\n"
        "ë¬¸ì¥ì— ìˆëŠ” ëª¨ë“  ê°œë…ì„ ë…¸ë“œë¡œ ë§Œë“¤ì–´ì¤˜"
        "ê° ë…¸ë“œì˜ descriptionì€ í•´ë‹¹ ë…¸ë“œë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì´ì–´ì•¼ í•´. "
        "ë§Œì•½ í…ìŠ¤íŠ¸ ë‚´ì— í•˜ë‚˜ì˜ ê¸´ descriptionì— ì—¬ëŸ¬ ê°œë…ì´ ì„ì—¬ ìˆë‹¤ë©´, ë°˜ë“œì‹œ ê°œë… ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì—¬ëŸ¬ ë…¸ë“œë¥¼ ìƒì„±í•´ì¤˜. "
        "descriptionì€ í•˜ë‚˜ì˜ ê°œë…ì— ëŒ€í•œ ì„¤ëª…ë§Œ ë“¤ì–´ê°€ì•¼ í•´"
        "ë…¸ë“œì˜ labelê³¼ nameì€ í•œê¸€ë¡œ í‘œí˜„í•˜ê³ , ë¶ˆí•„ìš”í•œ ë‚´ìš©ì´ë‚˜ í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ê°€í•˜ì§€ ë§ì•„ì¤˜. "
        "ë…¸ë“œì™€ ì—£ì§€ ì •ë³´ê°€ ì¶”ì¶œë˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë°°ì—´ì„ ì¶œë ¥í•´ì¤˜.\n\n"
        "json í˜•ì‹ ì™¸ì—ëŠ” ì¶œë ¥ ê¸ˆì§€"
        f"í…ìŠ¤íŠ¸: {chunk}"
        )
        try:
            # ì‘ë‹µ í¬ë§·ì„ JSONìœ¼ë¡œ ê°•ì œí•˜ì—¬ íŒŒì‹± ì•ˆì •ì„± í™•ë³´
            completion = client.chat.completions.create(
                model=self.model_name,  # ë™ì  ëª¨ë¸ ì„ íƒ
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì•¼. ì—£ì§€ì˜ sourceì™€ targetì€ ë°˜ë“œì‹œ ë…¸ë“œì˜ nameì„ ì°¸ì¡°í•´ì•¼ í•´."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=5000,
                temperature=0.3,
                # JSONë§Œ ëŒë ¤ì£¼ë„ë¡ ê°•ì œ
                response_format={"type": "json_object"}
            )

            # print("response: ", response)
            # data = json.loads(response)
            # print("data: ", data)
            # â¬‡ï¸  ë¬¸ìì—´ë§Œ ì¶”ì¶œ!
            content = completion.choices[0].message.content.strip()
            data = json.loads(content)
                
                
            # ê° ë…¸ë“œì— source_id ì¶”ê°€ ë° êµ¬ì¡° ê²€ì¦
            valid_nodes = []
            for node in data.get("nodes", []):
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                if not all(key in node for key in ["label", "name"]):
                    logging.warning("í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ëœ ë…¸ë“œ: %s", node)
                    continue

                # descriptions í•„ë“œ ì´ˆê¸°í™”
                if "descriptions" not in node:
                    node["descriptions"] = []
                    
                # source_id ì¶”ê°€
                node["source_id"] = source_id
                
                # description ì²˜ë¦¬
                if "description" in node:
                    node["descriptions"].append({
                        "description": node["description"],
                        "source_id": source_id  # ê° descriptionì—ë„ source_id ì¶”ê°€
                    })
                    del node["description"]
                    
                valid_nodes.append(node)
            
            # ì—£ì§€ì˜ sourceì™€ targetì´ ë…¸ë“œì˜ nameì„ ì°¸ì¡°í•˜ëŠ”ì§€ ê²€ì¦
            valid_edges = []
            node_names = {node["name"] for node in valid_nodes}
            for edge in data.get("edges", []):
                if "source" in edge and "target" in edge and "relation" in edge:
                    if edge["source"] in node_names and edge["target"] in node_names:
                        valid_edges.append(edge)
                    else:
                        logging.warning("ì˜ëª»ëœ ì—£ì§€ ì°¸ì¡°: %s", edge)
                else:
                    logging.warning("í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ëœ ì—£ì§€: %s", edge)

             # 1) ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬(ì˜ë¯¸ì— ë”°ë¼ì„œ ì—¬ëŸ¬ë¬¸ì¥ì¼ ìˆ˜ë„ ìˆìŒ)
            sentences = manual_chunking(chunk)   # List[str]
            logging.warning("ì²­í‚¹ëœ ë¬¸ì¥: %s", sentences)
            if not sentences:
                # ë¹ˆ sentencesì¸ ê²½ìš°ì—ë„ original_sentences í•„ë“œ ì¶”ê°€
                for node in valid_nodes:
                    node["original_sentences"] = []
                return valid_nodes, valid_edges

            # 2) ëª¨ë“  ë¬¸ì¥ ì„ë² ë”© (í•œ ë²ˆë§Œ)
            sentence_embeds = np.vstack([encode_text(s) for s in sentences])  # (num_sentences, dim)

            # 3) ë…¸ë“œë³„ë¡œ original_sentences ê³„ì‚°
            threshold = 0.8
        
            for node in valid_nodes:
                # node["descriptions"] ì—ëŠ” ë°˜ë“œì‹œ 1ê°œì˜ ë”•ì…”ë„ˆë¦¬ê°€ ë“¤ì–´ìˆë‹¤ê³  ê°€ì •
                desc_obj = node["descriptions"][0]
                desc_text = desc_obj["description"]
                desc_src  = desc_obj["source_id"]
                desc_text_full = f"{node['name']} - {desc_text}"
                # 3-1) name-description í˜•íƒœë¡œ ì„ë² ë”©
                desc_vec = np.array(encode_text(desc_text)).reshape(1, -1)  # (1, dim)

                # 3-2) ë¬¸ì¥ Ã— ì„¤ëª… ìœ ì‚¬ë„ ê³„ì‚°
                sim_scores = cosine_similarity(sentence_embeds, desc_vec).flatten()  # (num_sentences,)
                # 0.75 ì´ìƒ ë¬¸ì¥ ì¸ë±ìŠ¤Â·ì ìˆ˜ ëª¨ìœ¼ê¸°
                above = [(i, score) for i, score in enumerate(sim_scores) if score >= threshold]
                # 3-3) threshold ì´ìƒì¸ ë¬¸ì¥ë§Œ ëª¨ì•„ì„œ original_sentences ì— ì €ì¥
                node_originals = []
                
                if above:
                    # 0.8 ì´ìƒì¸ ëª¨ë“  ë¬¸ì¥ ì¶”ê°€
                    for i, score in above:
                        node_originals.append({
                            "original_sentence": sentences[i],
                            "source_id":        desc_src,
                            "score":            round(float(score), 4)
                        })
                else:
                    # 0.8 ë¯¸ë§Œ ì¤‘ ìµœê³ ì  ë¬¸ì¥ í•˜ë‚˜ë§Œ ì¶”ê°€
                    best_i = int(np.argmax(sim_scores))
                    node_originals.append({
                        "original_sentence": sentences[best_i],
                        "source_id":        desc_src,
                        "score":            round(float(sim_scores[best_i]), 4)
                    })


                # 3-4) ê° nodeì— í•„ë“œë¡œ ì¶”ê°€
                node["original_sentences"] = node_originals

            return valid_nodes, valid_edges 
        except Exception as e:
            logging.error(f"ì²­í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return [], []

    def _remove_duplicate_nodes(self, nodes: list) -> list:
        """ì¤‘ë³µëœ ë…¸ë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤.

        - ë™ì¼ (name, label) ì¡°í•©ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê³ , descriptionsëŠ” ë³‘í•©í•©ë‹ˆë‹¤.
        """
        seen = set()
        unique_nodes = []
        for node in nodes:
            node_key = (node["name"], node["label"])
            if node_key not in seen:
                seen.add(node_key)
                unique_nodes.append(node)
            else:
                # ê°™ì€ ì´ë¦„ì˜ ë…¸ë“œê°€ ìˆìœ¼ë©´ descriptionsë§Œ ì¶”ê°€
                for existing_node in unique_nodes:
                    if existing_node["name"] == node["name"] and existing_node["label"] == node["label"]:
                        existing_node["descriptions"].extend(node["descriptions"])
        return unique_nodes

    def _remove_duplicate_edges(self, edges: list) -> list:
        """ì¤‘ë³µëœ ì—£ì§€ë¥¼ ì œê±°í•©ë‹ˆë‹¤. (source, target, relation) ë™ì¼ ì‹œ í•˜ë‚˜ë§Œ ìœ ì§€"""
        seen = set()
        unique_edges = []
        for edge in edges:
            edge_key = (edge["source"], edge["target"], edge["relation"])
            if edge_key not in seen:
                seen.add(edge_key)
                unique_edges.append(edge)
        return unique_edges

    def generate_answer(self, schema_text: str, question: str) -> str:
        """
        ì§€ì‹ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ AIë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        logging.info("ğŸš€ OpenAI API í˜¸ì¶œ - ëª¨ë¸: %s", self.model_name)
        
        prompt = (
        "ë‹¤ìŒ ì§€ì‹ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, ì»¨í…ìŠ¤íŠ¸ì— ëª…ì‹œëœ ì •ë³´ë‚˜ ì—°ê²°ëœ ê´€ê³„ë¥¼ í†µí•´ ì¶”ë¡  ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œë§Œ ìì—°ì–´ë¡œ ë‹µë³€í•´ì¤˜. "
        "ì •ë³´ê°€ ì¼ë¶€ë¼ë„ ìˆë‹¤ë©´ í•´ë‹¹ ë²”ìœ„ ë‚´ì—ì„œ ìµœëŒ€í•œ ì„¤ëª…í•˜ê³ , ì»¨í…ìŠ¤íŠ¸ì™€ ì™„ì „íˆ ë¬´ê´€í•œ ê²½ìš°ì—ë§Œ 'ì§€ì‹ê·¸ë˜í”„ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ì¶œë ¥í•´. "
        "ì§€ì‹ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ í˜•ì‹:\n"
        "1. [ê´€ê³„ ëª©ë¡] start_name -> relation_label -> end_name\n (ëª¨ë“  ë…¸ë“œê°€ ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒì€ ì•„ë‹˜)"
        "2. [ë…¸ë“œ ëª©ë¡] NODE: {node_name} | DESCRIPTION: {desc_str}\n"
        "ì§€ì‹ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸:\n" + schema_text + "\n\n"
        "ì§ˆë¬¸: " + question + "\n\n"
        "ì¶œë ¥ í˜•ì‹:\n"
        "[ì—¬ê¸°ì— ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸ ë‹µë³€ ì‘ì„± ë˜ëŠ” 'ì§€ì‹ê·¸ë˜í”„ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.' ì¶œë ¥]\n\n"
        "EOF\n"
        "{\n"
        '  "referenced_nodes": ["ë…¸ë“œ ì´ë¦„1", "ë…¸ë“œ ì´ë¦„2", ...]\n'
        "}\n"
        "â€» 'referenced_nodes'ì—ëŠ” ì°¸ê³ í•œ ë…¸ë“œ ì´ë¦„ë§Œ ì •í™•íˆ JSON ë°°ì—´ë¡œ ë‚˜ì—´í•˜ê³ , ë„ë©”ì¸ ì •ë³´, ë…¸ë“œ ê°„ ê´€ê³„, ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆ."
        "â€» ë°˜ë“œì‹œ EOFë¥¼ ì¶œë ¥í•´"

     
        )


        try:
        
            response = client.chat.completions.create(
                model=self.model_name,  # ë™ì  ëª¨ë¸ ì„ íƒ
                messages=[{"role": "user", "content": prompt}]
            )
            response = response.choices[0].message.content

            print("response: ", response)
            final_answer = response
            return final_answer
        except Exception as e:
            logging.error("GPT ì‘ë‹µ ì˜¤ë¥˜: %s", str(e))
            raise RuntimeError("GPT ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
    

    def generate_schema_text(self, nodes, related_nodes, relationships) -> str:
        """
        ìœ„: start_name -> relation_label -> end_name (í•œ ì¤„ì”©, ì¤‘ë³µ ì œê±°)
        ì•„ë˜: ëª¨ë“  ë…¸ë“œ(ê´€ê³„ ìˆë“  ì—†ë“ ) ì¤‘ë³µ ì—†ì´
            {node_name}: {desc_str}
        desc_strëŠ” original_sentences[].original_sentenceë¥¼ ëª¨ì•„ ê³µë°± ì •ë¦¬ ë° ì¤‘ë³µ ì œê±°
        """


        def to_dict(obj):
            """ì…ë ¥ ê°ì²´ë¥¼ dictë¡œ ê´€ìš©ì ìœ¼ë¡œ ë³€í™˜(Neo4j ë ˆì½”ë“œ/ê°ì²´ í˜¸í™˜ìš©)."""
            try:
                if obj is None:
                    return {}
                if hasattr(obj, "items"):
                    return dict(obj.items())
                if isinstance(obj, dict):
                    return obj
            except Exception:
                pass
            return {}

        def normalize_space(s: str) -> str:
            """ì—°ì† ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ì •ê·œí™”."""
            return " ".join(str(s).split())

        def filter_node(node_obj):
            """ë…¸ë“œ ë ˆì½”ë“œ/ê°ì²´ì—ì„œ name/label/original_sentencesë§Œ ì¶”ì¶œ/ì •ê·œí™”."""
            d = to_dict(node_obj)
            name = normalize_space(d.get("name", "ì•Œ ìˆ˜ ì—†ìŒ") or "")
            label = normalize_space(d.get("label", "ì•Œ ìˆ˜ ì—†ìŒ") or "")
            original_sentences = d.get("original_sentences", []) or []
            parsed = []
            # ë¬¸ìì—´ì´ë©´ JSON íŒŒì‹± ì‹œë„
            if isinstance(original_sentences, str):
                try:
                    original_sentences = [json.loads(original_sentences)]
                except Exception:
                    original_sentences = []
            # ë¦¬ìŠ¤íŠ¸ ìš”ì†Œë“¤ ì •ê·œí™”
            for item in original_sentences:
                if isinstance(item, str):
                    try:
                        obj = json.loads(item)
                        if isinstance(obj, dict):
                            parsed.append(obj)
                    except Exception:
                        continue
                elif isinstance(item, dict):
                    parsed.append(item)
            return {"name": name, "label": label, "original_sentences": parsed}

        logging.info(
            "generating schema text: %dê°œ ë…¸ë“œ, %dê°œ ê´€ë ¨ ë…¸ë“œ, %dê°œ ê´€ê³„",
            len(nodes) if isinstance(nodes, list) else 0,
            len(related_nodes) if isinstance(related_nodes, list) else 0,
            len(relationships) if isinstance(relationships, list) else 0,
        )

        # 1) ëª¨ë“  ë…¸ë“œ ìˆ˜ì§‘ (name í‚¤ë¡œ í•©ì¹˜ê¸°)
        all_nodes = {}
        if isinstance(nodes, list):
            for n in nodes or []:
                if n is None: continue
                nd = filter_node(n)
                if nd["name"]:
                    all_nodes[nd["name"]] = nd
        if isinstance(related_nodes, list):
            for n in related_nodes or []:
                if n is None: continue
                nd = filter_node(n)
                if nd["name"] and nd["name"] not in all_nodes:
                    all_nodes[nd["name"]] = nd

        # 2) ê´€ê³„ ì¤„ ë§Œë“¤ê¸°: "start -> relation -> end"
        relation_lines = []
        connected_names = set()
        if isinstance(relationships, list):
            for rel in relationships:
                try:
                    if rel is None:
                        continue
                    start_d = to_dict(getattr(rel, "start_node", {}))
                    end_d   = to_dict(getattr(rel, "end_node", {}))
                    start_name = normalize_space(start_d.get("name", "") or "ì•Œ ìˆ˜ ì—†ìŒ")
                    end_name   = normalize_space(end_d.get("name", "") or "ì•Œ ìˆ˜ ì—†ìŒ")

                    # relation label: props.relation ìš°ì„ , ì—†ìœ¼ë©´ type, ì—†ìœ¼ë©´ "ê´€ê³„"
                    try:
                        rel_props = dict(rel)
                    except Exception:
                        rel_props = {}
                    relation_type = getattr(rel, "type", None)
                    relation_label = rel_props.get("relation") or relation_type or "ê´€ê³„"
                    relation_label = normalize_space(relation_label)

                    relation_lines.append(f"{start_name} -> {relation_label} -> {end_name}")
                    connected_names.update([start_name, end_name])
                except Exception as e:
                    logging.exception("ê´€ê³„ ì²˜ë¦¬ ì˜¤ë¥˜: %s", e)
                    continue

        # ê´€ê³„ ì¤‘ë³µ ì œê±° + ì •ë ¬
        relation_lines = sorted(set(relation_lines))

        # 3) ë…¸ë“œ ì„¤ëª… ë§Œë“¤ê¸°: ëª¨ë“  ë…¸ë“œ(ê´€ê³„ ì—¬ë¶€ ë¬´ê´€)
        def extract_desc_str(node_data):
            # original_sentences[].original_sentence ëª¨ì•„ ê³µë°± ì •ë¦¬ + ì¤‘ë³µ ì œê±°
            seen = set()
            pieces = []
            for d in node_data.get("original_sentences", []):
                if isinstance(d, dict):
                    t = normalize_space(d.get("original_sentence", "") or "")
                    if t and t not in seen:
                        seen.add(t)
                        pieces.append(t)
            if not pieces:
                return ""
            s = " ".join(pieces)
            
            return s

        node_lines = []
        for name in sorted(all_nodes.keys()):  # âœ… ê´€ê³„ ì—†ì–´ë„ ëª¨ë“  ë…¸ë“œ ì¶œë ¥
            nd = all_nodes.get(name) or {}
            desc = extract_desc_str(nd)
            if desc:
                node_lines.append(f"{name}: {desc}")
            else:
                node_lines.append(f"{name}:")  # ì„¤ëª…ì´ ë¹„ë©´ ì½œë¡ ë§Œ

        # 4) ìµœì¢… ì¶œë ¥: ìœ„ì—” ê´€ê³„ë“¤, ì•„ë˜ì—” ë…¸ë“œë“¤
        top = "\n".join(relation_lines)
        bottom = "\n".join(node_lines)

        if top and bottom:
            raw_schema_text = f"{top}\n\n{bottom}"
        elif top:
            raw_schema_text = top
        elif bottom:
            raw_schema_text = bottom
        else:
            raw_schema_text = "ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        logging.info("ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (%dì)", len(raw_schema_text))
        return raw_schema_text


    def chat(self, message: str) -> str:
        """
        ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¥¼ Ollama LLMì— ë³´ë‚´ê³ ,
        ëª¨ë¸ ì‘ë‹µ ë¬¸ìì—´ë§Œ ë¦¬í„´í•©ë‹ˆë‹¤.
        """
        try:
            resp = self.client.chat.completions.create(
                model=self.model_name,  # ë™ì  ëª¨ë¸ ì„ íƒ
                messages=[{"role": "user", "content": message}],
                stream=False
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            logging.error(f"chat ì˜¤ë¥˜: {e}")
            raise
