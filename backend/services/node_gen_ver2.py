"""embedding ëª¨ë¸ì„ KoE5ë¡œ ë³€ê²½í•˜ê³  ëŒ€ëª…ì‚¬ì„± ëª…ì‚¬êµ¬ flagë¥¼ í†µí•´ì„œ 
conference resolutionì„ ì‹œë„í•©ë‹ˆë‹¤.
ìœ ì‚¬í•œ ëª…ì‚¬êµ¬ë“¤ ì¤‘ ê°€ì¥ ê¸¸ì´ê°€ ê¸´ ëª…ì‚¬êµ¬ê°€ ëŒ€í‘œ ëª…ì‚¬êµ¬ê°€ ë©ë‹ˆë‹¤.
sliding window ë°©ì‹ì„ ì‚¬ìš©í•˜ë©°, 
ê° noun phraseê°€ ì²« ë“±ì¥í•œ ë¬¸ì¥ì˜ contextë§Œ ë°˜ì˜í•˜ëŠ” ì˜¤ë¥˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤."""


from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging

#pip install konlpy, pip install transformers torch scikit-learn

import re
import torch
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

MODEL_NAME = "nlpai-lab/KoE5"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)
model.eval()

stopwords = set([
    "ì‚¬ì‹¤", "ê²½ìš°", "ì‹œì ˆ", "ë‚´ìš©", "ì ", "ê²ƒ", "ìˆ˜", "ë•Œ", "ì •ë„", "ì´ìœ ", "ìƒí™©", "ë¿", "ë§¤ìš°", "ì•„ì£¼"
])

okt = Okt()

def is_pronoun_like_phrase(tokens: list[str]) -> bool:
    pronouns = {
        "ì´", "ê·¸", "ì €", "ë³¸", "í•´ë‹¹", "ê·¸ëŸ°", "ì „ìˆ í•œ", "ì•ì„œ", "ë§í•œ", "ì´ëŸ°", "ì´ëŸ¬í•œ", "ê·¸ëŸ¬í•œ"
    }
    
    if len(tokens) <2:
        return False
    
    check_len = min(2, len(tokens))
    for i in range(check_len):
        if tokens[i] in pronouns:
            return True
    return False


def sliding_windows(sentences: list[str], window_size: int = 2, stride: int = 1) -> list[str]:
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¬¸ë§¥ ì°½ì„ ë§Œë“­ë‹ˆë‹¤.
    
    Args:
        sentences (list[str]): ë¶„ë¦¬ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        window_size (int): í•œ ìœˆë„ìš°ì— í¬í•¨ë  ë¬¸ì¥ ìˆ˜
        stride (int): ë‹¤ìŒ ìœˆë„ìš°ë¡œ ì´ë™í•  ë¬¸ì¥ ìˆ˜

    Returns:
        list[str]: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í˜•íƒœë¡œ ë¬¶ì¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    windows = []
    for i in range(0, len(sentences) - window_size + 1, stride):
        window = sentences[i:i + window_size]
        windows.append(window)
    return windows

def extract_noun_phrases(text: str) -> list[dict]:
    """
    ëª…ì‚¬êµ¬ë¥¼ ì¶”ì¶œí•˜ê³ , ì§€ì‹œì–´(ëŒ€ëª…ì‚¬ì„±) ëª…ì‚¬êµ¬ ì—¬ë¶€ë¥¼ íŒë³„í•´ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        List of dicts like:
        [
            {"phrase": "RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ", "is_pronoun_like": False},
            {"phrase": "ì´ ì‹œìŠ¤í…œ", "is_pronoun_like": True}
        ]
    """
    words = okt.pos(text, norm=True, stem=True)


    noun_phrases = []
    current_phrase = []
    is_pronoun = False

    for word, tag in words:
        if '\n' in word:
            continue
        elif tag in ["Noun",  "Alpha"]:
            current_phrase.append(word)
        elif tag =='Adjective' and word[-1] not in 'ë‹¤ìš”ì£ ':
            current_phrase.append(word)
        else:
            if current_phrase:
                is_pronoun=is_pronoun_like_phrase(current_phrase)
                phrase = " ".join(current_phrase)
                noun_phrases.append({
                    "phrase": phrase,
                    "pronoun_like": is_pronoun
                })
                current_phrase = []
                is_pronoun = False

    if current_phrase:
        is_pronoun = is_pronoun_like_phrase(current_phrase)
        noun_phrases.append({
            "phrase": " ".join(current_phrase),
            "is_pronoun_like": is_pronoun
        })

    return noun_phrases

def get_embedding(text: str) -> np.ndarray:
    """
    KoE5 ëª¨ë¸ë¡œ ì…ë ¥ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ([CLS] ë²¡í„° ì‚¬ìš©)
    """
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í°ì˜ ë²¡í„°
    return cls_embedding.squeeze().cpu().numpy()

def compute_similarity_matrix(
    phrase_info: List[Dict],
    windows: List[List[str]],
) -> tuple[np.ndarray, List[str]]:
    """
    ëª…ì‚¬êµ¬ ë¦¬ìŠ¤íŠ¸ì™€ ìœˆë„ìš°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”©í•˜ê³  ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    """
    embeddings = []
    labels = []

    for item in phrase_info:
        phrase = item["phrase"]
        win_idx = item["window_index"]
        context = " ".join(windows[win_idx])

        # ë¬¸ë§¥ ë‚´ì—ì„œ ëª…ì‚¬êµ¬ ê°•ì¡°
        highlighted = context.replace(phrase, f"[{phrase}]")

        emb = get_embedding(highlighted)
        embeddings.append(emb)
        labels.append(phrase)

    sim_matrix = cosine_similarity(embeddings)

    return sim_matrix, labels

def group_phrases_and_replace(
    text: str,
    phrase_info: List[Dict],
    sim_matrix: np.ndarray,
    threshold: float = 0.98
) -> tuple[str, List[Dict]]:
    n = len(phrase_info)
    ungrouped = set(range(n))
    groups = []

    while ungrouped:
        i = ungrouped.pop()
        group = [i]
        to_check = set()

        # í›„ë³´: ì•„ì§ ê·¸ë£¹ì— ì†í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ ì¤‘ iì™€ ìœ ì‚¬ë„ê°€ threshold ì´ìƒì¸ ê²ƒ
        for j in ungrouped:
            if sim_matrix[i][j] >= threshold:
                to_check.add(j)

        # ì™„ì „ ì—°ê²°ëœ í´ëŸ¬ìŠ¤í„° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” jë§Œ ê·¸ë£¹ì— í¬í•¨
        valid_members = []
        for j in to_check:
            if all(sim_matrix[j][k] >= threshold for k in group):  # ê¸°ì¡´ ê·¸ë£¹ê³¼ ëª¨ë‘ ì—°ê²°ë˜ì–´ì•¼ í•¨
                valid_members.append(j)

        for j in valid_members:
            group.append(j)
            ungrouped.remove(j)

        groups.append(group)

    # ê·¸ë£¹ â†’ ëŒ€í‘œ ëª…ì‚¬êµ¬ ì„¤ì •
    phrase_map = {}
    group_infos = []
    for group in groups:
        phrases = [phrase_info[i]['phrase'] for i in group]
        rep_phrase = max(phrases, key=len)
        for i in group:
            phrase_map[phrase_info[i]['phrase']] = rep_phrase
        group_infos.append({
            "representative": rep_phrase,
            "members": phrases
        })

    # ê¸´ ëª…ì‚¬êµ¬ë¶€í„° ì¹˜í™˜
    sorted_phrases = sorted(phrase_map.keys(), key=len, reverse=True)
    replaced_text = text
    for phrase in sorted_phrases:
        replaced_text = replaced_text.replace(phrase, phrase_map[phrase])

    return replaced_text, group_infos

def normalize_coreferences(text: str) -> str:
    """
    ê°™ì€ ëŒ€ìƒì„ ì§€ì¹­í•˜ëŠ” ëª…ì‚¬êµ¬ë“¤ì„ ì²« ë²ˆì§¸ ëª…ì‚¬êµ¬ë¡œ í†µì¼
    """
    results=[]
    sentences = re.split(r'(?<=[.!?])\s+|(?<=[ë‹¤ìš”ì£ ì˜¤])\s*$', text.strip(), flags=re.MULTILINE)
    sentences=[s.strip() for s in sentences if s.strip()]
    windows = sliding_windows(sentences, window_size=2, stride=1)

    # ê° ë¬¸ì¥ì—ì„œ ëª…ì‚¬êµ¬ ì¶”ì¶œ
    for w_idx, window in enumerate(windows):
        if w_idx==0:
            phrases=extract_noun_phrases(window[0])
            s_idx=0
        else:
            phrases=extract_noun_phrases(window[1])
            s_idx=1
        s_idx += w_idx

        for p in phrases:
            results.append({
                "window_index": w_idx,
                "sentence_index": s_idx,
                "phrase": p["phrase"],
                "pronoun_like": p["pronoun_like"]
            })
    
    sim_matrix, labels = compute_similarity_matrix(results, windows)

    new_text, groups = group_phrases_and_replace(text, results, sim_matrix)

    print("âœ… ì¹˜í™˜ëœ í…ìŠ¤íŠ¸:")
    print(new_text)
    print("ğŸ“Œ ëª…ì‚¬êµ¬ ê·¸ë£¹ ì •ë³´:")
    for g in groups:
        print(f"{g['representative']} â† {g['members']}")
    
    return results

texts="""â‘  ì•¼ì„±ì , í™œë™ì , ì •ì—´ì 
ê³ ë ¤ëŒ€í•™êµì˜ êµí’ì€ ì•¼ì„±, í™œê¸°ì™€ ì •ì—´ ë“±ìœ¼ë¡œ ëŒ€í‘œëœë‹¤. ë¬´ì„­ê³  ì‚¬ë‚˜ìš´ í˜¸ë‘ì´, ê°•ë ¬í•˜ê²Œ ê²€ë¶‰ì€ í¬ë¦¼ìŠ¨ìƒ‰ ë“± ê³ ëŒ€ë¥¼ ëŒ€í‘œí•˜ê±°ë‚˜ 'ê³ ëŒ€' í•˜ë©´ ë– ì˜¤ë¥´ëŠ” ìƒì§•ë“¤ì€ ëŒ€ë¶€ë¶„ ìœ„ì˜ íŠ¹ì§•ë“¤ê³¼ ì—°ê´€ëœ ê²½ìš°ê°€ ë§ë‹¤. ì´ëŠ” ê³ ë ¤ëŒ€í•™êµê°€ ê·¸ ì „ì‹ ì¸ ë³´ì„±ì „ë¬¸í•™êµ ì‹œì ˆ ì‚¬ì‹¤ìƒ ìœ ì¼í•œ ë¯¼ì¡±Â·ë¯¼ë¦½ì˜ ì§€ë„ì ì–‘ì„±ê¸°êµ¬ì˜€ê¸° ë•Œë¬¸ì—, ë¯¼ì¡±ì •ì‹ ì´ë¼ëŠ” ì‹œëŒ€ì  ìš”êµ¬ê°€ êµìˆ˜ì™€ í•™ìƒë“¤ì—ê²Œ íŠ¹ë³„íˆ ë” ë¶€í•˜ëê³ , ê·¸ê²ƒì´ í•™ìƒë“¤ì˜ ì§€ì‚¬ì  ë˜ëŠ” íˆ¬ì‚¬ì  ì €í•­ ê¸°ì§ˆì„ ë°°íƒœì‹œì¼°ë˜ ë° ê¸°ì¸í•œë‹¤ëŠ” ê²¬í•´ê°€ ìˆë‹¤.[20]

â‘¡ í˜‘ë™ì , ëˆëˆí•¨
ê³ ë ¤ëŒ€ì—ì„œëŠ” ì¡¸ì—…ìƒì„ 'ë™ë¬¸', 'ë™ì°½' ë“±ì˜ ë‹¨ì–´ ëŒ€ì‹  'êµìš°(æ ¡å‹)'ë¼ê³  ë¶€ë¥¸ë‹¤. ì´ëŠ” í•™êµë¥¼ ê°™ì´ ë‹¤ë…”ë‹¤ëŠ” ì´ìœ ë§Œìœ¼ë¡œ ì¹œêµ¬ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. ì‚¬íšŒì—ì„œ ê³ ë ¤ëŒ€ ì¶œì‹  ê°„ì—ëŠ” ìœ ëŒ€ê°€ ë§¤ìš° ê°•í•œ í¸ì´ë©° ì´ëŸ¬í•œ ë¬¸í™”ëŠ” ê°œì¸ì£¼ì˜ ì„±í–¥ì´ ê°•í•´ì§„ í˜„ëŒ€ì—ë„ ì‚¬ë¼ì§€ì§€ ì•Šê³  ê±´ê°•í•˜ê²Œ ì´ì–´ì§€ê³  ìˆë‹¤. ê³ ëŒ€ì—ëŠ” ìê¸° ì´ìµë§Œ ì•ì„¸ìš°ë ¤ í•˜ê¸°ë³´ë‹¤ëŠ”, íƒ€ì¸ê³¼ ì†Œí†µí•˜ê³  ì„œë¡œì˜ ì¥ì ì„ ì‚´ë ¤ ì¼ì„ ë¶„ë‹´í•¨ìœ¼ë¡œì¨ ì‹œë„ˆì§€ë¥¼ ë‚´ëŠ” ë¬¸í™”ê°€ ë°œë‹¬ë¼ ìˆë‹¤. ë˜í•œ ì¼ëŒ€ì¼ ê°„ì˜ ê´€ê³„ë³´ë‹¤ëŠ” í­ë„“ì€ ì§‘ë‹¨ ë‚´ì—ì„œì˜ ê´€ê³„ë¥¼ ë” ì„ í˜¸í•˜ëŠ” í¸ì´ë‹¤.[21] êµ¬ì„±ì›ë“¤ì˜ ì• êµì‹¬ì´ ì›Œë‚™ ì»¤ì„œ ê·¸ëŸ°ì§€, ì •ì¹˜ì  ì´ë… ë° ê²½ì œì  ì´í•´ê´€ê³„ê°€ ë‹¤ë¥´ë”ë¼ë„ ê°™ì€ ê³ ëŒ€ ë™ë¬¸ ì‚¬ì´ì—ëŠ” ì¢€ ë” ìƒëŒ€ë°©ì˜ ì…ì¥ì— ì„œì„œ ìƒê°í•´ë³´ê³  ì¸ê°„ì  ì‹ ë¢°ì— ì…ê°í•˜ì—¬ ê°ˆë“±ì„ í’€ì–´ê°€ë ¤ëŠ” ì „í†µì´ ì´ì–´ì§€ê³  ìˆë‹¤. ì‹¤ì œë¡œ ê³ ë ¤ëŒ€ì—ëŠ” ë™ì•„ë¦¬ ì¡°ì§ì´ ë°œë‹¬í•´ ê·¸ êµ¬ì„±ì›ì´ ì¸ê°„ê´€ê³„ë¥¼ ë‹¤ì§€ê³  íŒ€í”Œë ˆì´ë¥¼ í•˜ëŠ” í’ì¡°ê°€ ê°•í•˜ë‹¤. ê³µë¶€ë„ ë¬¼ë¡  ì¤‘ìš”ì‹œí•˜ì§€ë§Œ, ê°œì¸ì˜ ì„±ì ë§Œì„ ì±™ê¸°ëŠ” ëŠ¥ë ¥ë³´ë‹¤ëŠ” ì¸ê°„ê´€ê³„ë¥¼ ì¶©ì‹¤íˆ í•˜ëŠ” ëŠ¥ë ¥, ë‚¨ì„ ì´ë„ëŠ” ì§€ë„ë ¥ì´ë‚˜ ìƒê¸‰ì, ë™ë£Œì™€ í™”í•©í•˜ëŠ” ì¹œí™”ë ¥ ë“±ì„ ë” ë†’ì´ í‰ê°€í•˜ëŠ” í¸ì´ë‹¤. ë‹¤ë¥¸ ê·¸ ë¬´ì—‡ë³´ë‹¤ë„ ì¥ê¸°ì ì¸ ëŒ€ì¸ê´€ê³„ì™€ ì‹ ë¢°ê°ì„ ì¤‘ì‹œí•˜ëŠ” ìŠµê´€, ì¡°ì§ì„ ìœ„í•´ í¬ìƒí•˜ê³  ë´‰ì‚¬í•˜ê³  ì˜¤ìš• ë’¤ì§‘ì–´ì“°ëŠ” ì¼ì„ ë‘ë ¤ì›Œí•˜ì§€ ì•ŠëŠ” ê¸°ì§ˆì´ ì´ëŸ° ë¬¸í™” ì†ì—ì„œ ê¸¸ëŸ¬ì§€ëŠ” ê±´ ë‹¹ì—°í•œ ì¼ì´ë‹¤.[22] 21ì„¸ê¸° ë“¤ì–´ì„œ ì˜¤í”„ë¼ì¸ ì»¤ë„¥ì…˜ë§Œì´ ì•„ë‹ˆë¼ ì˜¨ë¼ì¸ ì»¤ë„¥ì…˜ì˜ ì¤‘ìš”ì„±ì´ ë§¤ìš° ì»¤ì¡ŒëŠ”ë°, ì´ì— ë°œë§ì¶° ê³ ëŒ€ì—ì„œëŠ” ì¸í„°ë„· ì»¤ë®¤ë‹ˆí‹°ë„ ë§¤ìš° í™œë°œí•˜ê²Œ ìš´ì˜ë˜ê³  ìˆë‹¤. ê³ ë ¤ëŒ€í•™êµ ì—ë¸Œë¦¬íƒ€ì„ë„ ìƒë‹¹íˆ í™œì„±í™”ë˜ì–´ ìˆëŠ” í¸ì´ì§€ë§Œ, íŠ¹íˆ ê³ ëŒ€ì˜ ìë‘ ì¤‘ í•˜ë‚˜ì¸ ê³ íŒŒìŠ¤ì˜ ê²½ìš° ê°ì¢… ê²Œì‹œíŒì—ì„œ ìœ í†µë˜ê³  ëˆ„ì ë˜ëŠ” ì •ë³´ê°€ ë§¤ìš° ë°©ëŒ€í•  ë¿ ì•„ë‹ˆë¼ ì˜ì–‘ê°€ë„ ë†’ë‹¤.[23]"""
results=normalize_coreferences(texts)


"""
for item in results:
    if(item['pronoun_like']==True):
        print(item)
"""
