from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging

#pip install konlpy, pip install transformers torch scikit-learn

import re
import torch
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

MODEL_NAME = "nlpai-lab/KoE5"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)
model.eval()

okt = Okt()

def is_pronoun_like_phrase(tokens: list[str]) -> bool:
    pronouns = {
        "ì´", "ê·¸", "ì €", "ë³¸", "í•´ë‹¹", "ê·¸ëŸ°", "ì „ìˆ í•œ", "ì•ì„œ", "ë§í•œ", "ì´ëŸ°"
    }
    
    if len(tokens) <2:
        return False
    
    check_len = min(2, len(tokens))
    for i in range(check_len):
        if tokens[i] in pronouns:
            return True
    return False


def sliding_windows(sentences: list[str], window_size: int = 3, stride: int = 1) -> list[str]:
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¬¸ë§¥ ì°½ì„ ë§Œë“­ë‹ˆë‹¤.
    
    Args:
        sentences (list[str]): ë¶„ë¦¬ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        window_size (int): í•œ ìœˆë„ìš°ì— í¬í•¨ë  ë¬¸ì¥ ìˆ˜
        stride (int): ë‹¤ìŒ ìœˆë„ìš°ë¡œ ì´ë™í•  ë¬¸ì¥ ìˆ˜

    Returns:
        list[str]: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í˜•íƒœë¡œ ë¬¶ì¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    windows = []
    for i in range(0, len(sentences) - window_size + 1, stride):
        window = sentences[i:i + window_size]
        windows.append(window)
    return windows

def extract_noun_phrases(text: str) -> list[dict]:
    """
    ëª…ì‚¬êµ¬ë¥¼ ì¶”ì¶œí•˜ê³ , ì§€ì‹œì–´(ëŒ€ëª…ì‚¬ì„±) ëª…ì‚¬êµ¬ ì—¬ë¶€ë¥¼ íŒë³„í•´ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        List of dicts like:
        [
            {"phrase": "RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ", "is_pronoun_like": False},
            {"phrase": "ì´ ì‹œìŠ¤í…œ", "is_pronoun_like": True}
        ]
    """
    words = okt.pos(text, norm=True, stem=True)


    noun_phrases = []
    current_phrase = []
    is_pronoun = False

    for word, tag in words:
        if '\n' in word:
            continue
        elif tag in ["Noun",  "Alpha"]:
            current_phrase.append(word)
        elif tag =='Adjective' and word[-1] not in 'ë‹¤ìš”ì£ ':
            current_phrase.append(word)
        else:
            if current_phrase:
                is_pronoun=is_pronoun_like_phrase(current_phrase)
                phrase = " ".join(current_phrase)
                noun_phrases.append({
                    "phrase": phrase,
                    "pronoun_like": is_pronoun
                })
                current_phrase = []
                is_pronoun = False

    if current_phrase:
        is_pronoun = is_pronoun_like_phrase(current_phrase)
        noun_phrases.append({
            "phrase": " ".join(current_phrase),
            "is_pronoun_like": is_pronoun
        })

    return noun_phrases

def get_embedding(text: str) -> np.ndarray:
    """
    KoE5 ëª¨ë¸ë¡œ ì…ë ¥ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ([CLS] ë²¡í„° ì‚¬ìš©)
    """
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í°ì˜ ë²¡í„°
    return cls_embedding.squeeze().cpu().numpy()

def compute_similarity_matrix(
    phrase_info: List[Dict],
    windows: List[List[str]],
) -> tuple[np.ndarray, List[str]]:
    """
    ëª…ì‚¬êµ¬ ë¦¬ìŠ¤íŠ¸ì™€ ìœˆë„ìš°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”©í•˜ê³  ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    """
    embeddings = []
    labels = []

    for item in phrase_info:
        phrase = item["phrase"]
        win_idx = item["window_index"]
        context = " ".join(windows[win_idx])

        # ë¬¸ë§¥ ë‚´ì—ì„œ ëª…ì‚¬êµ¬ ê°•ì¡°
        highlighted = context.replace(phrase, f"[{phrase}]")

        emb = get_embedding(highlighted)
        embeddings.append(emb)
        labels.append(phrase)

    sim_matrix = cosine_similarity(embeddings)

    return sim_matrix, labels

def group_phrases_and_replace(
    text: str,
    phrase_info: List[Dict],
    sim_matrix: np.ndarray,
    threshold: float = 0.7
) -> tuple[str, List[Dict]]:
    """
    ìœ ì‚¬í•œ ëª…ì‚¬êµ¬ë¼ë¦¬ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê³ , ëŒ€í‘œ ëª…ì‚¬êµ¬ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¹˜í™˜í•¨.

    Args:
        text (str): ì›ë³¸ í…ìŠ¤íŠ¸
        phrase_info (List[Dict]): ëª…ì‚¬êµ¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (phrase, sentence_index, etc.)
        sim_matrix (np.ndarray): ëª…ì‚¬êµ¬ ê°„ ìœ ì‚¬ë„ í–‰ë ¬
        threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (default 0.7)

    Returns:
        replaced_text (str): ì¹˜í™˜ëœ í…ìŠ¤íŠ¸
        groups (List[Dict]): ê·¸ë£¹ ì •ë³´ (ëŒ€í‘œ ëª…ì‚¬êµ¬ì™€ í•˜ìœ„ ëª…ì‚¬êµ¬ë“¤)
    """

    n = len(phrase_info)
    used = set()
    groups = []

    # ê·¸ë£¹í•‘ (ìœ ì‚¬ë„ ê¸°ë°˜)
    for i in range(n):
        if i in used:
            continue
        group = [i]
        for j in range(i+1, n):
            if sim_matrix[i][j] >= threshold:
                group.append(j)
                used.add(j)
        used.add(i)
        groups.append(group)

    # ëŒ€í‘œ ëª…ì‚¬êµ¬ ì„ íƒ (ë¬¸ì¥ ì¸ë±ìŠ¤ê°€ ê°€ì¥ ì•ì„  ê²ƒ)
    phrase_to_rep = {}
    for group in groups:
        group_phrases = [phrase_info[i]["phrase"] for i in group]
        rep_phrase = min(group_phrases, key=lambda p: phrase_info[[pi["phrase"] for pi in phrase_info].index(p)]["sentence_index"])
        for p in group_phrases:
            phrase_to_rep[p] = rep_phrase

    # ê¸´ ëª…ì‚¬êµ¬ ë¨¼ì € ì¹˜í™˜ë˜ë„ë¡ ì •ë ¬
    sorted_phrases = sorted(phrase_to_rep.items(), key=lambda x: -len(x[0]))

    # í…ìŠ¤íŠ¸ ì¹˜í™˜
    for original, rep in sorted_phrases:
        if original != rep:
            text = re.sub(rf'\b{re.escape(original)}\b', rep, text)

    # ê·¸ë£¹ êµ¬ì¡° ë°˜í™˜
    group_info = []
    for group in groups:
        group_phrases = [phrase_info[i]["phrase"] for i in group]
        rep = phrase_to_rep[group_phrases[0]]
        group_info.append({
            "representative": rep,
            "phrases": group_phrases
        })

    return text, group_info

def normalize_coreferences(text: str) -> str:
    """
    ê°™ì€ ëŒ€ìƒì„ ì§€ì¹­í•˜ëŠ” ëª…ì‚¬êµ¬ë“¤ì„ ì²« ë²ˆì§¸ ëª…ì‚¬êµ¬ë¡œ í†µì¼
    """
    results=[]
    sentences = re.split(r'(?<=[.!?])\s+|(?<=[ë‹¤ìš”ì£ ì˜¤])\s*$', text.strip(), flags=re.MULTILINE)
    sentences=[s.strip() for s in sentences if s.strip()]
    windows = sliding_windows(sentences, window_size=3, stride=1)

    # ê° ë¬¸ì¥ì—ì„œ ëª…ì‚¬êµ¬ ì¶”ì¶œ
    for w_idx, window in enumerate(windows):
        for s_offset, sentence in enumerate(window):
            s_idx = w_idx + s_offset
            phrases = extract_noun_phrases(sentence)
            for p in phrases:
                results.append({
                    "window_index": w_idx,
                    "sentence_index": s_idx,
                    "phrase": p["phrase"],
                    "pronoun_like": p["pronoun_like"]
                })
    
    sim_matrix, labels = compute_similarity_matrix(results, windows)
    print(labels)

    new_text, groups = group_phrases_and_replace(text, results, sim_matrix)

    print("âœ… ì¹˜í™˜ëœ í…ìŠ¤íŠ¸:")
    print(new_text)
    print("ğŸ“Œ ëª…ì‚¬êµ¬ ê·¸ë£¹ ì •ë³´:")
    for g in groups:
        print(f"{g['representative']} â† {g['phrases']}")
    return results

texts="""â‘  ì•¼ì„±ì , í™œë™ì , ì •ì—´ì 
ê³ ë ¤ëŒ€í•™êµì˜ êµí’ì€ ì•¼ì„±, í™œê¸°ì™€ ì •ì—´ ë“±ìœ¼ë¡œ ëŒ€í‘œëœë‹¤. ë¬´ì„­ê³  ì‚¬ë‚˜ìš´ í˜¸ë‘ì´, ê°•ë ¬í•˜ê²Œ ê²€ë¶‰ì€ í¬ë¦¼ìŠ¨ìƒ‰ ë“± ê³ ëŒ€ë¥¼ ëŒ€í‘œí•˜ê±°ë‚˜ 'ê³ ëŒ€' í•˜ë©´ ë– ì˜¤ë¥´ëŠ” ìƒì§•ë“¤ì€ ëŒ€ë¶€ë¶„ ìœ„ì˜ íŠ¹ì§•ë“¤ê³¼ ì—°ê´€ëœ ê²½ìš°ê°€ ë§ë‹¤. ì´ëŠ” ê³ ë ¤ëŒ€í•™êµê°€ ê·¸ ì „ì‹ ì¸ ë³´ì„±ì „ë¬¸í•™êµ ì‹œì ˆ ì‚¬ì‹¤ìƒ ìœ ì¼í•œ ë¯¼ì¡±Â·ë¯¼ë¦½ì˜ ì§€ë„ì ì–‘ì„±ê¸°êµ¬ì˜€ê¸° ë•Œë¬¸ì—, ë¯¼ì¡±ì •ì‹ ì´ë¼ëŠ” ì‹œëŒ€ì  ìš”êµ¬ê°€ êµìˆ˜ì™€ í•™ìƒë“¤ì—ê²Œ íŠ¹ë³„íˆ ë” ë¶€í•˜ëê³ , ê·¸ê²ƒì´ í•™ìƒë“¤ì˜ ì§€ì‚¬ì  ë˜ëŠ” íˆ¬ì‚¬ì  ì €í•­ ê¸°ì§ˆì„ ë°°íƒœì‹œì¼°ë˜ ë° ê¸°ì¸í•œë‹¤ëŠ” ê²¬í•´ê°€ ìˆë‹¤.[20]

â‘¡ í˜‘ë™ì , ëˆëˆí•¨
ê³ ë ¤ëŒ€ì—ì„œëŠ” ì¡¸ì—…ìƒì„ 'ë™ë¬¸', 'ë™ì°½' ë“±ì˜ ë‹¨ì–´ ëŒ€ì‹  'êµìš°(æ ¡å‹)'ë¼ê³  ë¶€ë¥¸ë‹¤. ì´ëŠ” í•™êµë¥¼ ê°™ì´ ë‹¤ë…”ë‹¤ëŠ” ì´ìœ ë§Œìœ¼ë¡œ ì¹œêµ¬ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. ì‚¬íšŒì—ì„œ ê³ ë ¤ëŒ€ ì¶œì‹  ê°„ì—ëŠ” ìœ ëŒ€ê°€ ë§¤ìš° ê°•í•œ í¸ì´ë©° ì´ëŸ¬í•œ ë¬¸í™”ëŠ” ê°œì¸ì£¼ì˜ ì„±í–¥ì´ ê°•í•´ì§„ í˜„ëŒ€ì—ë„ ì‚¬ë¼ì§€ì§€ ì•Šê³  ê±´ê°•í•˜ê²Œ ì´ì–´ì§€ê³  ìˆë‹¤. ê³ ëŒ€ì—ëŠ” ìê¸° ì´ìµë§Œ ì•ì„¸ìš°ë ¤ í•˜ê¸°ë³´ë‹¤ëŠ”, íƒ€ì¸ê³¼ ì†Œí†µí•˜ê³  ì„œë¡œì˜ ì¥ì ì„ ì‚´ë ¤ ì¼ì„ ë¶„ë‹´í•¨ìœ¼ë¡œì¨ ì‹œë„ˆì§€ë¥¼ ë‚´ëŠ” ë¬¸í™”ê°€ ë°œë‹¬ë¼ ìˆë‹¤. ë˜í•œ ì¼ëŒ€ì¼ ê°„ì˜ ê´€ê³„ë³´ë‹¤ëŠ” í­ë„“ì€ ì§‘ë‹¨ ë‚´ì—ì„œì˜ ê´€ê³„ë¥¼ ë” ì„ í˜¸í•˜ëŠ” í¸ì´ë‹¤.[21] êµ¬ì„±ì›ë“¤ì˜ ì• êµì‹¬ì´ ì›Œë‚™ ì»¤ì„œ ê·¸ëŸ°ì§€, ì •ì¹˜ì  ì´ë… ë° ê²½ì œì  ì´í•´ê´€ê³„ê°€ ë‹¤ë¥´ë”ë¼ë„ ê°™ì€ ê³ ëŒ€ ë™ë¬¸ ì‚¬ì´ì—ëŠ” ì¢€ ë” ìƒëŒ€ë°©ì˜ ì…ì¥ì— ì„œì„œ ìƒê°í•´ë³´ê³  ì¸ê°„ì  ì‹ ë¢°ì— ì…ê°í•˜ì—¬ ê°ˆë“±ì„ í’€ì–´ê°€ë ¤ëŠ” ì „í†µì´ ì´ì–´ì§€ê³  ìˆë‹¤. ì‹¤ì œë¡œ ê³ ë ¤ëŒ€ì—ëŠ” ë™ì•„ë¦¬ ì¡°ì§ì´ ë°œë‹¬í•´ ê·¸ êµ¬ì„±ì›ì´ ì¸ê°„ê´€ê³„ë¥¼ ë‹¤ì§€ê³  íŒ€í”Œë ˆì´ë¥¼ í•˜ëŠ” í’ì¡°ê°€ ê°•í•˜ë‹¤. ê³µë¶€ë„ ë¬¼ë¡  ì¤‘ìš”ì‹œí•˜ì§€ë§Œ, ê°œì¸ì˜ ì„±ì ë§Œì„ ì±™ê¸°ëŠ” ëŠ¥ë ¥ë³´ë‹¤ëŠ” ì¸ê°„ê´€ê³„ë¥¼ ì¶©ì‹¤íˆ í•˜ëŠ” ëŠ¥ë ¥, ë‚¨ì„ ì´ë„ëŠ” ì§€ë„ë ¥ì´ë‚˜ ìƒê¸‰ì, ë™ë£Œì™€ í™”í•©í•˜ëŠ” ì¹œí™”ë ¥ ë“±ì„ ë” ë†’ì´ í‰ê°€í•˜ëŠ” í¸ì´ë‹¤. ë‹¤ë¥¸ ê·¸ ë¬´ì—‡ë³´ë‹¤ë„ ì¥ê¸°ì ì¸ ëŒ€ì¸ê´€ê³„ì™€ ì‹ ë¢°ê°ì„ ì¤‘ì‹œí•˜ëŠ” ìŠµê´€, ì¡°ì§ì„ ìœ„í•´ í¬ìƒí•˜ê³  ë´‰ì‚¬í•˜ê³  ì˜¤ìš• ë’¤ì§‘ì–´ì“°ëŠ” ì¼ì„ ë‘ë ¤ì›Œí•˜ì§€ ì•ŠëŠ” ê¸°ì§ˆì´ ì´ëŸ° ë¬¸í™” ì†ì—ì„œ ê¸¸ëŸ¬ì§€ëŠ” ê±´ ë‹¹ì—°í•œ ì¼ì´ë‹¤.[22] 21ì„¸ê¸° ë“¤ì–´ì„œ ì˜¤í”„ë¼ì¸ ì»¤ë„¥ì…˜ë§Œì´ ì•„ë‹ˆë¼ ì˜¨ë¼ì¸ ì»¤ë„¥ì…˜ì˜ ì¤‘ìš”ì„±ì´ ë§¤ìš° ì»¤ì¡ŒëŠ”ë°, ì´ì— ë°œë§ì¶° ê³ ëŒ€ì—ì„œëŠ” ì¸í„°ë„· ì»¤ë®¤ë‹ˆí‹°ë„ ë§¤ìš° í™œë°œí•˜ê²Œ ìš´ì˜ë˜ê³  ìˆë‹¤. ê³ ë ¤ëŒ€í•™êµ ì—ë¸Œë¦¬íƒ€ì„ë„ ìƒë‹¹íˆ í™œì„±í™”ë˜ì–´ ìˆëŠ” í¸ì´ì§€ë§Œ, íŠ¹íˆ ê³ ëŒ€ì˜ ìë‘ ì¤‘ í•˜ë‚˜ì¸ ê³ íŒŒìŠ¤ì˜ ê²½ìš° ê°ì¢… ê²Œì‹œíŒì—ì„œ ìœ í†µë˜ê³  ëˆ„ì ë˜ëŠ” ì •ë³´ê°€ ë§¤ìš° ë°©ëŒ€í•  ë¿ ì•„ë‹ˆë¼ ì˜ì–‘ê°€ë„ ë†’ë‹¤.[23]"""
results=normalize_coreferences(texts)


"""
for item in results:
    if(item['pronoun_like']==True):
        print(item)
"""
